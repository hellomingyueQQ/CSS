Course Overview
Course Overview
Hi everyone! My name is Filip Ekberg, and welcome to my course, Asynchronous Programming in C# 10. I'm a principal consultant and CEO at a consultant agency operating out of Gothenburg in Sweden. I started this company quite a few years ago, and focus on building fast, powerful, and easy‑to‑maintain solutions. This course is for those of you that want to build fast, responsive, and overall better applications by applying asynchronous principles. We'll cover all the topics that you need to understand how to safely and effectively introduce asynchronous and parallel programming in your applications. This includes introducing the async and await keywords and getting a complete understanding of how that affects the application and the potential problems you may run into, understanding how to use the task parallel library to introduce your own asynchronous operations, and how it fits into the bigger picture. You will explore parallel operations using parallel for, parallel for each, parallel invoke, and parallel link. Then, learn how to use parallel and asynchronous principles together. Finally, we'll go through some advanced topics, and you will explore the internals of async and await, thread safety, sharing resources, deadlocking, and atomic operations. All of this ultimately makes you appreciate all the effort the compiler is doing for you. By the end of this course, you will know how to effectively apply asynchronous and parallel programming in C# by using the async and await keywords and the task parallel library. This will give you a good understanding of how asynchronous programming affects the application, the common issues you may run into, and how to adapt to more complex scenarios. Before beginning this course, you should be familiar with programming in C#. I hope you'll join me on this journey to learn how to build fast, responsive, and powerful applications with the course Asynchronous Programming in C# 10 here at Pluralsight.

Getting Started with Asynchronous Programming in C# using Async and Await
Asynchronous Programming in .NET
Do you feel puzzled when it comes to asynchronous programming and how that applies to your .NET applications? Do you want to build better‑performing applications and write code that doesn't lock up the application while it's waiting for data to be available? Solving this is often where you will first encounter asynchronous programming. Understanding this can be quite tricky and overwhelming, but don't worry, you are in good hands. Throughout this course, you will go from learning the basics to understanding how to best apply asynchronous programming in your .NET applications. No matter if you are new to C# or an experienced C# developer, you will definitely benefit from this course. My name is Filip Ekberg, and welcome to the course Asynchronous Programming in C#. All the patterns and principles covered throughout this course can be applied in any type of .NET application. This is the application we are going to improve throughout this course. It doesn't yet use any asynchronous principles, and our job is to make the application better by introducing ways to load the data without locking the user interface. The purpose of this application is to provide you with fake stock price information. If you search for a stock, you can see that the user experience is rather bad. The application freezes, and after a while the operating system is even telling us that the application is no longer responding. Let's jump over to the code and find out why. This is a Windows application, but everything we talk about can be applied in any C# and .NET application. This application will make it easier to illustrate the benefits and importance of asynchronous programming. When the button in the application is pressed, the method Search_Click will be executed. This is the event handler for the click event. Some of the code in this method is considered legacy, but unfortunately, it's what a lot of us have to work with. The code uses a legacy code piece called the WebClient. The WebClient provides a way for us to synchronously go and download a string from a particular web API. Being synchronous means that it's going to run on the same thread that executes it. In our case, this method call originates from the same thread that manages the entire application graphical user interface, also known as the UI thread. This thread will be blocked until the synchronous call has completed. This means that we need a way to offload the work of loading that data to a different thread. And once the data has been properly loaded, we can go ahead and handle it. Let me quickly refactor the code and apply the asynchronous principles that you will learn all about throughout this course. Don't worry, we're going to cover everything that's going on here in greater detail throughout this entire course and apply everything step by step. With this refactored, the application now behaves a little bit differently. I can take notes as the data is being loaded. This is a big improvement to the user experience. The refactored and improved version of the application applies the asynchronous principles, which means that the heavy work of loading the data has been moved to a different thread. This will prevent the application from locking up and having more responsive UI. When the data has been loaded, the code to handle it is notified, and the UI can be updated accordingly. There's not really a big difference between the synchronous and asynchronous versions of the code. There are a few different classes and a few different keywords. Instead of using the old legacy WebClient, we are now introducing the class HttpClient, which provides a way to perform asynchronous operations to consume web resources. The HttpClient is used to get all the information for the stocks from a web API. We do so by calling the method GetAsync, which will perform a web request against the URL that we've specified as its parameter. This here is an asynchronous operation. Calling this method means that the operation will occur asynchronously. Therefore, the UI thread is relieved of the work to download the data and can instead listen for additional UI interactions. Apart from calling GetAsync, you can also see the async and await keywords being used in the code. Async is a way for you to indicate that this method will contain asynchronous operations. Await is a way to indicate that we want to come back to the code and execute everything after the await keyword only once the data is loaded from the API. We're going to master these keywords and how to use them appropriately and effectively throughout this entire course. The biggest reason for introducing asynchronous principles is to improve the user experience. It's also a way to write better‑performing applications as you can proceed to execute other parts of the code while you wait for an asynchronous operation to complete. We saw this in action when the UI thread was relieved of heavy work and was able to listen for more user input. Asynchronous and parallel programming isn't really a new concept. Traditionally, we've had different ways of building multi‑threaded and asynchronous code with, for instance, the lower‑level threading, or something like the background worker, which is an event‑based asynchronous pattern. Nowadays it's mostly common to use the Task Parallel Library together with the async and await keywords. With minimum effort, this synchronous code that locked up the application and gave us a really bad user experience was refactored into an application that applies these asynchronous principles. This gives the user a much better experience. This here is just scratching the surface, but shows why it's important to learn how to properly apply the asynchronous principles. I just want to emphasize that async and await, as well as the Task Parallel Library, is something that you can use in any different type of C# and .NET application. There's a lot that can go wrong if we don't properly understand how to apply these asynchronous principles. Proceed throughout this course to fully understand how it all works and fits together.

Version Check
This course was created using the latest version of C# and .NET. No preview versions or preview features have been used in this course. I'm using the latest version of Visual Studio on Windows. There's a free community edition available. Optionally, you can follow along in your favorite development environment on your preferred operating system. While this course uses the latest version of C# and .NET, many of the language features covered are applicable to older versions of C# ranging all the way back to C# 1. If you use an older version of the C# compiler, I cannot guarantee that all the features work as shown in this course. If you happen to run into any issues or have questions about backwards compatibility, leave a question on the discussion tab on the website. All the features and principles covered in this course can be applied in any type of .NET application, no matter if it's a console application, Windows application or ASP.NET Core website. Proceed throughout this course to learn all about asynchronous programming in C#.

Setting up the Exercise Files
You can easily follow along with the demos and exercises in this course. The course page includes a link to download all the exercise files. Download the ZIP folder containing all the files that you need for this course. These can also be downloaded from GitHub, which will ensure that you have the latest version of the exercise files. Extract these files wherever you want. I put my files in C:\Code as a too long path can often cause problems. Once you've extracted the ZIP file, you will get a Windows and a Cross‑Platform folder. If you follow along using macOS or Linux, choose the Cross‑Platform folder, and if you're on Windows, you would use the Windows folder. You'll find exercises for each module, where each folder is divided into a Completed and Start_Here folder to make it easy for you to follow along. If you run into any problems, you can always peek inside the Completed folder to see the fully completed solution for that exercise. When you open a new starting point, make sure that you first restore the NuGet packages and then rebuild the solution. This is to make sure that we don't run into any issues once we start the project. This course is focusing on applying asynchronous programming in C#, and we'll do most of our coding in the MainWindow.xaml.cs file. This course doesn't require you to know anything about UI programming. We will simply use this application to understand the principles, which are applicable in any C# and .NET application. I'm using Visual Studio on Windows throughout this course, but you can follow along in your favorite developer environment on your preferred operating system. If you run into any problems, feel free to comment on the discussion on the Pluralsight website and we'll make sure that you can follow along with the course. While taking this course, feel free to pause the video at any time and add the code to your project and fill out the exercise files. This way you will learn a lot more about how all these asynchronous principles work in a .NET application. With the exercises ready, let's proceed to learn everything you need to know about building powerful and performant applications by applying asynchronous programming in C#.

Introducing Async and Await in C#
In the first clip of this module, I quickly refactored the application to illustrate the benefits of asynchronous programming. I'm now going to undo all of that and step‑by‑step take you through the process of introducing your first asynchronous code. We're back working with the legacy web client. We certainly want to get rid of this mainly because it's executing the web call synchronously. It's blocking the application from doing other work while it's downloading the data. To make this method run asynchronously, a very common misconception is to simply add the async keyword. The keyword async is added right before the return type in the method signature. You might now think that the code inside this method will run asynchronously. Let's run the application and see if this had an impact on the performance. When you search for a fake stock price, it still locks up. According to the operating system, it's not responding. After a little while, the application becomes responsive again. This is because the data has now been received by the application and the synchronous operation is completed. Let's jump into the code and talk about why this is still not an asynchronous operation. We can see that Visual Studio provides a hint that tells you that this method is marked as async, but the method doesn't use the await keyword. Whenever you encounter the async keyword, you must have the await keyword inside the same method. Async is a prerequisite for the method to be capable of being asynchronous, but simply adding it isn't enough. Remember this because it's extremely important. To make it asynchronous, we must replace the synchronous web call with an asynchronous alternative. We'd like to use the HttpClient instead, as this is a newer and better alternative. It will provide methods to asynchronously interact with web resources. This means that we could download data without locking up the application. Asynchronous patterns and principles are not only for communicating with web resources. Asynchronous principles are suitable for any type of I/O operations. In this case, we interact with a web API, but asynchronous programming is also good for reading and writing from disk, memory, or things like database operations. Then what about CPU bound operations? For that, you'd use parallel programming, which is a way to divide a problem into smaller pieces that are all solved independently while making use of as much computer power as possible. While an asynchronous operation do occur in parallel, the distinct difference is that within asynchronous operation, you subscribe to when that operation completes. The Task Parallel Library allows for both asynchronous, as well as parallel programming. And you can even combine these principles. All right, back to the code. To fetch data from the web API and do so asynchronously, we are using to GetAsync method on the HttpClient. I'm hosting an API in the cloud, which you can use throughout this course. The URL for this API is already added to the exercise files. So what makes this call asynchronous? GetAsync returns a task of an HttpResponseMessage. The HttpResponseMessage is simply the result this operation produces. A task represents an asynchronous operation. The variable response task is a task, and therefore, a reference to the ongoing asynchronous operation. I bet you're wondering how to get the result produced by an asynchronous operation? If we explore the properties available on this task, you can see that there's one called Result. Using this is often the first thing that people try, and this is a really bad idea. What happens when you request the result using this property is that it will block the thread until the result is available. This means that it will run the code synchronously. Given that you probably understand why this is a bad idea, instead of calling responseTask.Result, how do we get the HttpResponseMessage, then only proceed executing the rest of the method when that operation has completed? This is where we introduce the await keyword. Await is a way to indicate that we want to get the result from the asynchronous operation once the data is available, and most important of all, without blocking the current thread. As you see, adding the await gives you the HttpResponseMessage. This value will be available as soon as the web request has completed. Then to get the content of the web request, you have to read that once the response is received. There is a method called ReadAsStringAsync on the HttpResponseMessage. This is also an asynchronous operation. If you call ReadAsStringAsync and then use the result property, this would block and make the code run synchronously. A word of warning here, calling the Result property or even the Wait method could potentially deadlock the application. Avoid using them if you can. This code retrieves the HttpResponseMessage from the GetAsync operation and puts that into the response variable. The variable will be set once the response is available. Only once that operation is completed, the next line will be executed. All of this without blocking the UI thread. Whenever you use the await keyword, the variable that we have on the left‑hand side will always contain the result of the asynchronous operation. The API returns a JSON response, so before running the application, make sure that this is deserialized into an object that we can use. The list of stock prices can now be added to the data grid in the application. It's time to run the application, and this should now perform a lot better than it did earlier. If you set a breakpoint in the method, then run the application with the debugger attached, you can follow the way that this executes the code. When you reach the await, the control will be returned to the UI thread, and the user can continue working in the application. Once the asynchronous operation has completed, the next line after the await will continue to execute. The asynchronous principles that we've applied in this application is really not only meant for Windows applications or other types of UI applications; we can also apply the same principles on the server side code in ASP.NET. Go ahead and open the ASP.NET web project inside the solution. Go to the file HomeController inside this web project and then find the Index action. We can paste the same code that we had in the Windows application, which retrieves the fake stock information. Let's hard‑code a StockIdentifier to make it a little bit more simple. To make it compile, we must change the method signature off this action to async Task of ActionResult. Because the method uses the await keyword, therefore, we must also have an async keyword present. The fact that this returns a task of ActionResult means that ASP.NET will have a reference to the ongoing asynchronous operation, and when the result is available, it can continue executing to get the ActionResult from the action. Whenever you have a task, you pretty much always want to await that in the same method, mainly because it will give you the result of the asynchronous operation, but it will also make sure that the call was completed successfully and that there were no exceptions. We will return the decentralized list of stock prices to the view, which will then be processed. And finally, the client that called this ASP.NET website will get a result. Running this gives an interesting result. Using async and await in ASP.NET does not make the client asynchronous. It might seem like it's a little bit different from how it worked in the Windows application. It took a little while to load the stock data, and you did notice that it wasn't, in fact, an asynchronous operation from the client's point of view. Then, what's the point of using async and await on the server? The big benefit of using async and await inside ASP.NET is to relieve the web server from work. The web server can then go ahead and work with other requests as data is being loaded from disk, the database, memory, or from another API. So far we've looked at communicating with a web resource, but as previously mentioned, these asynchronous principles are appropriate for other types of operations as well. Head over to the class library in this solution, which has a bit of pre‑built functionality. There's a class here called DataStore, which uses a local file for the stock price lookup. Instead of querying the web resource, you could consider this being an offline cache. Inside the data store, there's a method called GetStockPrices. The method signature for GetStockPrices indicate that this is an asynchronous method. We can determine this by seeing that it returns a task of t. It also uses the async keyword, which means that somewhere in this method, there's an asynchronous call, which this method expects a result from. The purpose of this method is to interact with the file system in an asynchronous manner. Inspecting this further, we can see that it opens a stream to a file on disk and asynchronously requests each line in that file. If you want to consume this method, you'd use it very much like you did with the HttpClient. Head back to MainWindow.xaml.cs, remove all the usage of the HttpClient, and add a new instance of the DataStore class. To query for a particular stock, call the method GetStockPrices. This returns a task of a list of stock prices, and you should now know that to get the result from this asynchronous operation, you use the await keyword. Once the stock prices have been loaded from disk, you'll get the result, and it can be added to the data grid from the application to make sure that it still behaves the same. Since all the data is available locally, it probably won't take much time to load the stocks, given the file isn't too large. You have now successfully introduced the async and await keywords in a C# application. You saw that the asynchronous principles are powerful, no matter if you're working in ASP.NET, in Windows, or other types of C# and .NET applications. While this is a good starting point, there's still a lot to discover. Adding these keywords might seem simple and trivial, but we have only scratched the surface.

Understanding a Continuation
Up until now, we've talked about marking the methods as async, introducing the await keyword, and the fact that the asynchronous operations introduce a separate thread where the work is being done. This avoids locking up the UI thread or the main thread of the application. This might seem like magic, and you're probably wondering now what the await really does and how it affects the flow of the code. As you discovered in the previous clip, introducing the await keyword allows you to retrieve the result from an asynchronous operation when that result is available. What it also does is to make sure that there were no exceptions or problems with the task that's being awaited. Therefore, it's a way to get the result, as well as validate that asynchronous operation. What the await keyword also does is introducing something called a continuation. The continuation is what will be executed after the asynchronous operation, meaning the code after the await keyword. This code will run once the task has completed, and it will run on the same thread that spawned the asynchronous operation, which in our case is the UI thread. This can be configured, but we'll get to that later. In our case, we perform an asynchronous operation to go ahead and retrieve some stock information. As we have the await keyword in front of the task that represents this asynchronous operation, all of the code after the await will be executed in what's called a continuation. You're not limited to just one continuation. In fact, it's pretty common that a method may contain more than one await. We saw this earlier when using the HttpClient. Each await in its turn introduces a continuation for the code after it. You can have as many awaits as you want inside your method marked with the async keyword. They will all introduce a continuation, which allows you to make sure that the code after the await keyword is only executed once the asynchronous operation that you are awaiting completed and did so successfully. If GetStockPrices can't find the stock we're searching for, it will throw an exception. We only want to update the UI with a list of stock prices if they could be loaded. If they couldn't, we'd like to give the user a hint that something went wrong. We can wrap the code in a try block and catch any exceptions that could occur. The await keyword will validate that the operation completed successfully. But if we search for a stock that doesn't exist, the task it's awaiting will be marked as faulted. What happens when the await keyword sees that the task faulted with an exception? It'll throw that back to the current thread. This means you'd end up in a catch block, and you could give the user a note of what's happening. We can run the application and see that if we search for a stock that doesn't exist, we'll get an error in the notes section of the application that tells us that there was an error. In this code block, the code that would update the UI with a list of stock prices and the code inside the catch block are both in the continuation of the asynchronous operation and will be executed on the UI thread. The await keyword ensures that we are back on the original context, which in this case is the UI thread. Every time I refer to the original context, I refer to the UI thread or the thread that started the asynchronous operation. Just as we were updating the data grid when we were asynchronously getting stock data, we can interact with Notes.Text to update it with an appropriate error message. This shows how the continuation allows for work to happen on the original context that spawned the asynchronous operation. If you've ever done multithreaded programming in UI applications before, you know that updating the UI from different threads isn't allowed. Therefore, it's so important to know that the default behavior when using await is that you will be back on the original thread in the continuation. We now understand that after the await keyword, we're inside something called a continuation. And when using the await keyword, the continuation allows you to perform work on the original context. This means that we don't have to worry about working across different threads. It makes our lives a whole lot easier. Not only does the await keyword allow you to retrieve the result from an asynchronous operation, it is also for validation to make sure there were no problems. It validates the task by checking if there were any exceptions. Comparing this to other approaches that you might have seen in the past, this makes it a whole lot cleaner, easier to maintain, and ultimately provides a better user experience. So far we've used pre‑existing asynchronous methods. But what about if we'd like to introduce our own? Continue to the next clip to learn about that.

Creating Your Own Asynchronous Method
We all know that we shouldn't really add too much code inside our methods. We should keep our methods clean. I recommend that we refactor our code just a little bit. At the same time, also learn about introducing our own asynchronous methods. You're now going to take all the code from the method Search_Click and move that into a new one. Then we'll call this method from the Search_Click event handler. The first thing is to introduce the new method. Let's call it GetStocks. This is going to be an asynchronous method that will be awaited inside Search_Click. Nowadays, it's less common to suffix methods with the async word because a lot of editors will hint you that you need to await the task that is returned. Therefore, we'll keep the name simple. For the implementation, we have two options. Either GetStocks returns the data that it's loading from the API or disk. Alternatively, as with the code we copied from Search_Click, we could interact with the UI from this method as well. If we choose to do all the work inside GetStocks, we could just paste the code from Search_Click. Before we do that, let's talk about a few things first. To introduce an asynchronous method, we know that we first introduce the async keyword. Marking that method as async simply indicates that this method will have the capability of executing and awaiting asynchronous operations. Remember, if we don't have the await keyword, we do not need the async keyword, either. Currently, the method signature indicates that this method doesn't return anything. It's a simple void method. Adding the async keyword means that this is now async void. Whenever you see this, you should really be worried. Async void is evil and is something that you should always avoid if you can. How come you're allowed to do async void if it's such an evil combination? The reason is event handlers. If you want to have asynchronous code in your event handlers, the delegate for an event handler is most certainly returning void. Therefore, the only time you are allowed to do async void is for event handlers. Keep that in mind because it's really important. The method GetStocks is a normal method and not an event handler. How would you indicate that this is an asynchronous method that doesn't return anything without using async void? Instead of having the async void method signature, you can use async Task. Changing the method signature to async Task didn't require any change to the method body, and it didn't produce any compilation errors. In fact, adding the async keyword will make sure that a task is automatically returned. The compiler will do this for us. The task will be a reference to the ongoing asynchronous operation, which you could await wherever this method is called. We'll talk more about the task and its many useful methods in the next module. Then, later in the course we'll talk about what the compiler generates, which might seem a little bit like magic. With the method signature set to async Task, this now allows you to populate this method with asynchronous operations. The caller of this method will have a reference to the ongoing operations and can await that to make sure that it completed successfully. Let's add the code from Search_Click. This will load the stock data. It will ensure that there were no exceptions and finally populate the data grid. If a problem occurs, we'll get a note in the notes section of the application. What's important here is that the only asynchronous portion of this code is the retrieval of data. The code that updates the UI is not going to be executed asynchronously. This means that if we were to perform a heavy operation before or after the await keyword, this isn't something that's offloaded to a separate thread. The code after the await keyword will be executed in the continuation, which would be the UI thread. You've introduced your first asynchronous method. To whoever calls this method, they will see that it returns a task and know that it performs something asynchronously. While at this point we only consume existing libraries to introduce an asynchronous call, you will later learn how to perform heavy work on a separate thread together with the async and await keywords. When we introduced this method, we made sure to follow the best practices of not returning void from a method that's using the async keyword. You could not call this method from Search_Click. The event handler still needs to be marked with the async keyword. That's because GetStocks is an asynchronous operation which we'd like to await. The method returns a task, and we need to make sure that this operation completed successfully, and therefore, you introduce the await keyword. The code was refactored, we've cleaned it up a little bit, and made sure that we have a minimum amount of code inside the async void method. Run this and you will see that it still works. We can search for stocks, it'll populate the data grid, and if you search for a stock that doesn't exist, a note will be added to the application to tell you there was a problem. I would say that it was super easy for us to refactor the code into a new asynchronous method. I mentioned earlier that the compiler takes care of returning the reference to the ongoing operation for a method marked with the async keyword. And earlier we talked about that the await keyword is something that helps us get the result out of an operation, and it's there to help us validate that the operation completed successfully. While GetStock doesn't return anything itself, the returned task is generated and added by the compiler, this needs to be awaited in whoever calls this method. Making sure that this is awaited is sometimes referred to using async all the way up. At the top level, in this case, the click event handler is where we want to await this asynchronous operation. If there's a problem inside the asynchronous operation, we need to make sure that it's validated and that potential exceptions aren't lost somewhere along the way. If you set a breakpoint at the top of Search_Click and trigger the event, we can step through the code and see how it behaves. This will execute on the UI thread. When stepping into GetStocks, this code is still being executed on the UI thread. As we reach the call to start an asynchronous operation to load some data, a separate thread is being created where it loads all the data. When you reach the await keyword, all the code beneath it will be executed in the continuation only once the data is available. Therefore, you can see that you're passed back to the method Search_Click, and the code until the next await keyword will be executed. Once that is reached, a new continuation is introduced. Finally, when all the data is available, the first code that is executed is the rest of the code in GetStocks, followed by the rest of the code in Search_Click after the await keywords. This shows that it's super simple for you to introduce your own asynchronous method. We refactor this to make the code a little bit more readable. Remember that we could apply the same approach inside ASP.NET, a console application, MAUI, or really any type of C# application.

Handling Exceptions
Given everything that we've looked at so far, asynchronous programming seems handy and it's something that we really want to introduce in our applications, especially if we want to increase the user experience. Although, sometimes the asynchronous operations might fail. We did talk about the fact that the await keyword is a way to validate that the asynchronous operations perform successfully. When we call GetStockPrices, an error occurs, the await keyword makes sure that this is propagated back to the caller. This is why we can catch the exception and add a note to the user that something went wrong. Have you considered what happens if you simply remove the await keyword? You'll notice that nothing happens. The application behaves as if the stocks were loaded, but we can't see anything about a failure. This proves that if we don't use the await keyword, the exceptions will be swallowed. The exceptions will be swallowed by the task which is referencing the ongoing operation. This task will be in a false state with an attached exception. Without the await keyword validating that this task completed successfully, we don't know what happened. I also mentioned earlier that async void is something that you'd really want to avoid. Let me now show you why that is. When you mark a method as async, it will automatically return a task; obviously, if that's what the method signature indicates. But if we mark the method as async void while it compiles and runs, there's now a big difference in how it works. Run this without the debugger attached, then search for a non‑existing company, and you'll see that the application just terminates. It totally crashed because what happened was that the exception that was thrown inside the method couldn't be set on a task. This means there's no task to swallow the exception and the exception will be thrown back to the caller. This is problematic. Being great developers as we are, we simply try and catch this inside the click event handler. We might now expect the application to simply swallow the exception, and do as it did before, and tell us that it loaded the stock, and add a note to the user, and not populate anything inside the data grid. It's still tearing down the application. The problem here is async void. Async void is really terrible, and if you use it, your applications may crash if there's an unhandled exception inside your asynchronous method. So how do we fix this? Well, an easy solution is for you to not write any code that causes exceptions, and you'll be totally fine. Although, that's not really realistic. Let's revert the method signature back to async Task. In the case of Search_Click, this can't be anything but async void; therefore, there are instead two things that we can do to ensure that exceptions are caught and handled. First, we need to await the asynchronous method call. Second, we should wrap the code in a try and catch block. As a side note, for methods like this that needs to be marked as async void, try to minimize the amount of code that could potentially fail. What we've done here is to make sure that no code inside our async void method can throw an exception. This will just minimize the risk of it being a problem. Ideally, we'd avoid using async void completely, but in this case, we just can't. Remember to always use the await keyword to validate that there's no exceptions in your asynchronous operations. We've got a better experience again when running the application. Searching for a non‑existing stock immediately shows us that there's an error that occurred. Without the await keyword present, we wouldn't have been able to catch this exception in the method. If you have a long chain of asynchronous operations, you need to make sure that it's awaited somewhere. At this point, we don't want the call to be fired and forgotten. This would swallow potential exceptions, and we'll never know if there was a problem. I can't stress enough how important this is, avoid using async void and always make sure that you await your asynchronous operations.

Best Practices
As we progressed throughout this module, you learned all that you need to know in order to start working with async and await in C#. Remember, it doesn't matter if you're working in WPF, WinForms, Windows services, Xamarin, console applications, or ASP.NET. you can use these patterns and principles in all these different types of .NET applications. This means that you can apply asynchronous principles across all your different types of applications. And if you use libraries that exposes asynchronous operations, you now know how to consume that. The biggest difference between the type of application we've been working with in this module and something like ASP.NET is that ASP.NET doesn't have a UI. Instead, what happens is that in ASP.NET, we relieve the web server of work so that it can take care of other requests while your asynchronous operations are running. We've looked at a whole lot of interesting things in this module, and we've learned some best practices. Introduce the async keyword for the method to be capable of being asynchronous, but simply adding the async keyword is not enough. When calling a method that returns a task or a task <T>, use the await keyword to introduce a continuation that will allow you to consume the result, as well as validating that there were no exceptions thrown in the asynchronous operation. The async and await keywords should always be used together. One simply doesn't make any sense without the other. Make sure that you mark your methods as async Task or async Task<T>. You want to make sure that you avoid using async void unless it's for event handlers. If you do end up using async void, make sure that the code inside your method doesn't throw any exceptions. I know this is really hard, but just make sure that you wrap your code in try and catch blocks so that you can recover from potential problems. Keep in mind that methods calling an async void method won't be able to await the asynchronous operation. Also, avoid using the result property or calling the Wait method on a task. Instead, introduce the await keyword. If you change the call to GetStock and use Wait instead, this will look like it's behaving like asynchronous call. In fact, this performs a lot worse because it's actually deadlocking the application, and this is a much bigger problem. When you use the Result property or call the Wait method on your asynchronous operations, not only are you potentially locking the application until that data is available, but you might also end up deadlocking, and this is a terrible user experience. Just make sure that you avoid the Result property or the Wait method. As a side note, I do want to mention that in the continuation after the await keyword, it would be totally fine to use the result property. This module has given you an introduction to how to use async and await in your .NET applications. Try to follow these best practices covered in this summary when applying async and await. If you mark your methods as async make sure that you use the await keyword, otherwise, you'll just introduce a lot of unnecessary generated code without really making anything asynchronous. You also want to make sure that you await your chain of asynchronous operations to make sure that they all completed successfully. This is often referred to as using async and await all the way up. I hope you enjoyed this module where you got an introduction to async and awaiy in .NET. There are still a ton of things that we need to cover in this course for you to fully understand how to apply asynchronous programming in your applications. Next up, we'll talk in more depth about the task and how we can work with that. We'll cover how it's a little bit different using the continuation on a task, rather than using the await keyword. Later, we'll deep dive into asynchronous programming, the async and await keywords, as well as some best practices. I really hope that you are as excited as I am to proceed in this course.

Using the Task Parallel Library for Asynchronous Programming
Introducing the Task
Welcome back to the course, Asynchronous Programming in C#. I'm Filip Ekberg, and now we are going to talk in detail about using the Task Parallel Library in .NET. The Task Parallel Library, commonly also referred to as the TPL. In this module, we'll talk about how to introduce tasks in our applications. This will essentially allow you to run heavy operations on a different thread. Without using the async and await keywords, you will discover ways to obtain the result, work with the exceptions, and run code after these operations complete. You will even see how you can introduce different continuations, depending on if the task succeeded or failed. You will also learn about canceling a task, which may sound a lot easier than it actually is. To explore and fully understand tasks, we're going to implement a way to load some file contents from our disk. First, we'll do this in the normal synchronous way, and then we'll proceed to do this asynchronously. The DataStore included in the project already implements a way to asynchronously work with a file system. But let's take an alternative approach to learn about tasks. Let's continue to do all our work in Search_Click, and for now, we are going to remove the call to GetStocks, which we introduced in the previous module, as well as removing the async and await keywords. Provided with this project is a comma separated file, which we are going to open and then parse. You can use the file class to read all the lines from a file. Each line will then be parsed and stored in a list of stock prices. We are going to iterate over each line in the file, but we'll skip the first row because that contains the headers. I've included a helper to parse the comma‑separated values into a StockPrice instance. This can now be added to the list of stock prices. We can then use link to find the stock identifiers that we're looking for, and finally, the data grid in the application can be updated with the stock prices. There's really nothing that we've written now that's asynchronous. If you run the application, you can see that it takes a bit of time to load all the data and populate the data grid. While it loaded the data, the application freezes, and you're unable to perform additional work. The reason for this is because you load a very large file without using an asynchronous approach. This is what takes the most of the time. All of this runs on the UI thread. No wonder the application is locking up. To make this asynchronous, coming from the first module, the first thing that you probably want to check is if there's an asynchronous version of ReadAllLines available on the file class. In this case, we are in luck. As of a few versions ago, .NET includes an asynchronous version of this method. We are going to assume that this does not exist and explore alternative approaches as, in a lot of cases, we might not be as lucky to have an asynchronous alternative available. There are alternative ways in the .NET framework to allow you to work with the file system in an asynchronous manner. However, in a lot of cases, this is situations that you have to work with. You end up using libraries that provide a lot of great functionality, but they don't expose any asynchronous versions of their methods. Therefore, we need a way to introduce asynchronous principles without relying on the libraries to provide this for us out of the box. So how do we approach this? One way is to introduce a task. We encountered the task in the previous module, but we didn't spend too much time talking about it. The task is a way to represent an asynchronous operation. It exposes functionality to allow you to perform all the necessary operations that you'll need to. Here are a few things the task allows you to do. You can schedule work to be executed on a different thread. It provides a way to get the result from that asynchronous operation, you can subscribe to when an operation is done by introducing a continuation, it can even tell you if there was an exception. There's much more to the task than this, but we'll get to that later. The task provides a static method called run. This method is used to queue work on what's called a thread pool. Simply put, the work will be executed on a different thread. You can point it to a method, pass a function, or an action that will run on that different thread. There are two versions of Task.Run, the generic version that will have a return value after it has completed. We don't need to explicitly use the generic version because if the task returns something, the compiler can infer the type. Task.run returns an instance of a task that represents the asynchronous operation. Continue to the next clip to learn how to apply this new knowledge of Task.run to move the synchronous version of the code into a new asynchronous operation.

Creating an Asynchronous Operation Using a Task
Withthisinmind,howaboutthatwetakeallthecodethatwe'vejustwrittenandmovethatintoanewanonymousmethodthatwepassintoTask.Run. This will queue the code to execute on a separate thread. Unless the thread pool is too busy, this should start off immediately. The whole purpose of calling Task.Run is to start work without locking the current thread or the context. This is totally awesome. There are a few problems though. When you run the application, it will tell you that it took 0 ms to load all the data. In reality, we know that it doesn't. A bigger issue is that it didn't populate the data grid with any data. This is an indication that something might not be working as it should. If we run this application with the debugger attached, it will tell you why this didn't work. An exception is being thrown because we are trying to update the UI from a different thread than the UI thread. The object that we are trying to access is owned by the UI thread and the task is running on another one. We need a way to communicate with the UI thread while still allowing for the majority of the heavy work to run on a separate thread. Coming from the previous module, you'd probably want to add the async and await keywords, but let's try to solve this without introducing them. There's another way for us to queue work on the UI thread from the task. In WPF, you can use something called a dispatcher, and it has an invoke method to queue and run the work passed to it on the UI thread. In WinForms, Xamarin, and other UI frameworks, there are similar ways to do this. Under the hoods, this builds on something known as the synchronization context. Even in non‑UI applications, you have ways to communicate across threads. When converting synchronous code, that we've just looked at, into asynchronous code, we need to make sure that the code that we have inside our asynchronous version don't queue any heavy work on the UI thread. This would ultimately defeat the purpose. As an example, don't execute long running operations using dispatcher.invoke. You want to be able to pretty much guarantee that what you pass to a Task.Run don't necessarily go back and block the UI thread. When we search for a stock now, the application behaves much better, and after a little while, all the data is available in the data grid. There's a small bug in the application. It's still giving us the incorrect amount of milliseconds it took to load the data. I know for a fact that it took much longer than what it's telling us. What happens here is that the task scheduler will queue this work onto the thread pool, and it will just execute that whenever there's an available thread. The call to Task.Run will therefore complete immediately, as the work is being sent somewhere else. Therefore, the next line will execute, and the timer that calculates how long this operation took will fire off straight away. Don't worry, we'll fix this. If you already have an idea on how to solve this, I'd recommend pausing the video and giving that a try. To make sure that the code after Task.Run only execute once the asynchronous operation has completed, you by now know that you have to introduce a continuation. You've hopefully watched the previous module and know that we can now introduce the async and await keywords. Don't worry, in the next clip, we'll explore how to do this without using async and await. To summarize what happens in this code block, we queue all the work to be executed somewhere else, and when we are notified that the work is done, we go ahead and run the code after the await keyword. This will give the user a pretty good experience. Once all the data is available, we'll populate the data grid, and you can continue to take notes as the data is being loaded. While the application behaves similar to what we did at the end of the previous module, we've now introduced an alternative way to something asynchronously, especially given the fact that the code block that we converted was previously synchronous and locked up the entire application. You saw how easy it is to introduce the task to schedule work to be executed somewhere else. In a lot of cases, you might have code that takes a little bit of time to run or use libraries that don't expose asynchronous versions. In these cases, this is the perfect fit for where to introduce Task.Run to execute code in an asynchronous operation. You're now also familiar with how to combine your own task with the async and await keywords. While async and await makes the process of creating a continuation and obtaining a result much easier, we'd still like to explore how to do this just using the task. So let's explore that in the next clip.

Obtaining the Result of a Task
We are now going to get rid of the async and await keywords, again, simply because I want to explore an alternative approach of introducing a continuation and obtaining a result. We have two expensive operations in this code block. The first one reads all the lines from the file, which obviously takes a little bit of time, especially if the file turns out to be really large. Then there is the process of parsing and processing each line, and this operation may take a little bit of time as well. This is why we'd like to avoid running these two operations on the UI thread. We can refactor the code a little bit to split these two operations into multiple steps. The first task will load all the lines asynchronously from the file system, and the task will then return its content. Whoever then subscribes to when this task completes will be able to grab the result inside the continuation. This task represents an asynchronous operation that when it's done, will return an array of strings. We could optionally use the generic version of Task.Run to explicitly say that we are expecting this to be a task of array of string. However, that's inferred by the compiler, so we don't have to explicitly say that here. At this point, we could use the async and await keywords, but as mentioned earlier, we want to do this without using async and await. Instead, you can chain on a continuation onto a task with a method called ContinueWith. This creates a continuation that executes asynchronously when the target task is completed. This means that the continuation is also executed on a different thread. Just as we did with Task.Run, we can represent the operation that we want to run using an anonymous method. The method executed by ContinueWith accepts a parameter. This will take the task that just completed. So inside the continuation, you can access the task that was just completed and execute this continuation. Do you remember the result property that I told you to never use? You're going to use that now. If you recall, I did mention that you can use the result property if the value is available. If it's not available and you call result, it will block until that operation completes, but when we were inside the continuation, the task will have ran to completion and there will be a result available. Therefore, it doesn't log any threats. The result property contains what the task returns. In our case, this will be an array of strings. We'll iterate this array of strings and produce a StockPrice instance for each element. The continuation is now chained onto the task and will run as soon as it completes. What's interesting with ContinueWith is that it, in fact, returns a task as well. It could therefore return a result, but at the same time, it could also throw an exception that we'd have to make sure that we capture somewhere in the application. It also means that you could chain on another continuation to the task that is returned by continue with which also returns a task and you could chain on another one to that one. Then you could change on another one, and so forth. You could add as many continuations as you'd like. Each one would execute the task that it's chained on once that task completes. A task can even have multiple continuations that will all execute once the task completes. Since ContinueWith is an asynchronous operation, it also means it will run on a separate thread, so to invoke the UI, in our case, we have to use the dispatcher. We'd like to avoid the same bug we saw earlier, where it said that the operations completed super‑fast, but at the same time, avoid introducing the async and await keywords. You can do so by chaining on a final continuation. In this case, I can simply use a discard to say that I don't care about the parameter. That will run the last portion of the method on the UI thread. What we've achieved now is that we've introduced an asynchronous operation that will load the file content from disk. When completed, the array of strings will be returned. We attach a continuation that will convert each of these lines into the stock price instances and then populate the UI. In a final continuation, we then remove the loading indicator from the application. The application is now as good as the one that we had at the end of the previous module, but without using any of the async and await keywords. While this is an alternative approach to subscribing to an asynchronous operation, and when it's completing, it also shows how much you get out of the box when using async and await. If we compare this to the async and await approach, this here is a little bit more noisy. There's a lot more code, I would say that it's more error prone, and it doesn't really read as nice as the async and await version did. There is also a difference in that the continuation in this case is executed on a different thread, which, as we saw, could be a little bit problematic. When we wanted to interact with the UI from the continuation, we had to introduce the dispatcher for every time we want to invoke anything on the UI. Sometimes introducing the async and await keywords is a little bit unnecessary. As we will learn later in this course, it does introduce a lot of generated code. In those situations, an alternative approach where we can only use things in the Task Parallel Library might therefore be needed. If we try to load a file that doesn't exist, nothing happens in the application, except the final continuation did, in fact, remove the loading indicator and updated how long it took to execute all of the code. You notice here that it did try to load the stocks, that task failed, then it's executing its continuation. This is really a big difference from how the async and await approach worked. This just shows you that there's still a long way to go to make this as good as our async and await version was.

Nested Asynchronous Operations
Before we move on and talk about handling success and failure, I want you to know that these asynchronous operations could spawn asynchronous operations themselves. Really, any anonymous methods could use the async and await keywords. This means that the anonymous method that Task.Run and ContinueWith use could be marked as async and introduce continuations inside of them using the await keyword. Keep in mind, if you do, the continuation within the asynchronous operation will execute on the same thread as your task did, not on the UI thread. That is extremely important. Let's say that we found a way to read a file asynchronously. We could then easily change the code and use the async and await keywords without having to change much else in the application. We could use the same approach as the data store uses to read files asynchronously. In the anonymous method that reads the file, let's use this alternative approach that we've seen inside the data store. We're going to start off by opening a stream to a file that we have on disk. This provides us with a way to asynchronously read each line in that file. To be capable of awaiting each line, we must mark the anonymous method as async. Marking an anonymous method as async means that the anonymous method will automatically return a task, so don't worry, this won't be async void. This is now using the async and await keywords inside the task itself. Using Task.Run, we created an asynchronous operation that would run somewhere else. This operation is now capable in its turn to create and properly await other asynchronous operations. When all the lines have been read from the file, the anonymous method can return this list back to whoever is listening for the result. This can then be processed inside the continuation. The application still allows us to search for the stocks. We combined running code and a separate thread using a task, as well as inside the task used the async and await keywords. To see how we could interact with other asynchronous code, it illustrates that if we create an asynchronous operation, that operation, in its turn, can create new asynchronous operations. Next up, we'll talk more about continuations and how you can have separate continuations for success or failure. Handling errors is really a good idea, and chaining on a final continuation that only executes once there's a problem would be appropriate. Understanding that and how to capture and handle exceptions will be covered next.

Handling Task Success and Failure
In most applications, it's a good idea to handle exceptions, even if they're not expected to happen. Most exceptions and problems occur because they're unexpected. How about if we'd like to execute code when there's a successful execution of our asynchronous operation and not execute it if there was a problem? In our code that loads the file from disk, when we change the final link to a non‑existing file, and when we try to open that and read the lines, an exception will be thrown. If that happens, it's unnecessary to run the following continuation which handles the parsing and populating the data grid. If we execute the application with the debugger attached and try to load a file that doesn't exist, we can see that the task inside the continuation is in a fault state. The promise of a continuation is to be executed once an asynchronous operation is completed. With the task and the continuation introduced with ContinueWith, it will do so no matter if it was a successful operation or not. The anonymous method requests the completed task to be passed into it, and upon inspection, you can see that the task has an attached exception. The inner exception indicates that it couldn't load the file. Since we are using the Task Parallel Library and not using async and await, the exception that we get here is an aggregate exception. This means to get any valuable information out of the exception on this particular task, you have to look at the inner exception, you will see that it will continue to execute the next continuation as well. In the final continuation, we don't capture the task, and all that that is doing is updating the UI. If we were to attach another continuation onto the task that's now updating the UI, the task passed into that wouldn't in fact be a fault state because the previous task that it's continuing from did so successfully. So now you are probably wondering if there's a way for us to indicate that we only want to execute this code when the previous operation that we're continuing from completed successfully without any exceptions. When creating the continuation, there's something called continuation options, it allows you to specify the behaviors to the scheduler for when to execute this continuation. You could say that you should only execute this continuation when the status of the task it's continuing from is not faulted. We could also simply say that we want to execute this when the task ran to completion, meaning there were no exceptions. If we specify that this should be executed if the task we were continuing from ran to completion, it will only execute the continuation if the task has no exceptions and was not canceled. Running this again with the debugger, you'll see that it skips the continuation, which is marked as only to be executed when the task ran to completion. How about if we'd like to add a continuation that handles exceptions? If you just append another ContinueWith at the end of the method chain here, you'd probably not get the result you expect. The task passed into the continuation would be the wrong task, as it would point to the task which represents the previous continuation. That's not our intent. Our goal is to add a continuation that handles exceptions when the task that processes our alliance is in a fault state. Don't worry, the final continuation will execute no matter if the previous task was executed. This will ensure that the loading indicator is always removed from the UI. If we explore the TaskContinuationOptions, we could use OnlyOnFaulted or NotOnRanToCompletion. Let's go with OnlyOnFaulted for now. There's now a specific continuation that adds a note to the application with the message of the inner exception. This will only execute If there was a problem completing the task. You could have multiple continuations attached to a task that runs for the particular purpose. You could control if one or more of these would execute depending on different scenarios using the task continuation options, the notes are now updated to say that it couldn't find the file we are looking for, obviously because the file doesn't exist. And with very minimum effort, we chained on a continuation that the scheduler will only execute if the task it's processing faulted. This allows us to have multiple different continuations that matches the expected outcome of the asynchronous operation. Prior to the async and await keywords, this was a much more common approach, and there's nothing stopping you from combining the two. As an example, let's say that you want to log the completion of a task. You could simply chain on a continuation with ContinueWith. And to conform with the same method signature that you were might expecting, you can return the value that was passed into the continuation. That way, it returns the same type of task with a given result that you previously expected. So this won't change the method signature or how the consumer awaits the task that is returned. They won't even know that it executed this continuation. A big difference is that the exceptions in the continuation of ContinueWith will contain an aggregate exception. With the await, it doesn't give you an aggregate exception, thus you don't have to inspect the inner exception to get any valuable information out of that. I always recommend that you use async and await, especially to validate that your tasks have completed successfully, so you will be able to catch potential exceptions. This will also give you the cleanest code. If you already have multiple different continuations in your project and you want to make sure that one of them is only executed when there's a problem or when there's no problem, you could use the task continuation options. Remember that ContinueWith also returns a task and runs asynchronously. If there's a problem in the code, it'll throw an exception, and then it will execute the next continuation in line. You've now seen how to introduce a chain of tasks where some will be executed upon exceptions and some only when everything completed successfully. No matter if you're using async and await, or the Task Parallel Library with ContinueWith, make sure that you validate your tasks and your asynchronous operations. It's definitely important to also validate the continuations because none of us wants unexpected behaviors in our applications. You can now handle the success or failure by either using async and await or the Task Parallel Library. This is something every developer should be doing when introducing asynchronous code in their applications. Next up, we'll be talking about canceling a task which may sound a lot simpler than it actually is.

Cancellation and Stopping a Task
In some cases, you want to cancel a long‑running operation. This could be an API call, the process of loading a large file from disk, or really, anything that you wrap in a task. In our application, for example, maybe we start the process of searching for a stock, but we immediately see that we've entered the incorrect company, and therefore we'd like to correct this by allowing the user to cancel the search and correct the mistake, because I know this goes ahead and loads a large file and that could be a pretty expensive operation if the file is really big. It's rather unnecessary to force the user to wait for that operation to complete. In an ideal scenario, if we canceled the operation, the data that it has already loaded should definitely be displayed in the UI. To allow a task to be canceled, there are a few things that needs to be done. First, we need to introduce a member inside our class of type CancelationTokenSource. This is what we are going to use to indicate that we want to cancel an operation. This class exposes methods for signaling for cancelation. It even provides functionality to indicate that you want to cancel after a certain amount of time. We'll introduce this CancelationTokenSource as a member in our class. We don't want this limited by the scope of the click event handler. The cancelationTokenSource is coupled with something called a CancelationToken. The CancelationToken is what is passed into an asynchronous operation. This token can be used to indicate to a task that it's canceled, but it can also be used within an asynchronous operation to check if a cancelation has been requested. In our case, we are going to use the same button for searching, as well as allowing the user to cancel the search. I reckon this is a pretty common pattern. If there's already an instance created for the cancelationTokenSource. That means we've already started a search, and the button has been clicked before. If that's the case, we know that something's currently performing a search. We'll then signal that we want to cancel all the operations that use the CancelationToken produced by this cancelationTokenSource. Then we'll also set the cancelationTokenSource to null. We'll reset the Search button's text and return from the method, because obviously, we wouldn't want to continue running the code beneath to start a new search. That would kind of defeat the purpose of canceling it. This code takes care of the case of when the Search button is clicked for a second time. If it's the first time we hit Search after a previous search has completed, we'd like to run the code below that handles loading and displaying the stocks. Before initializing the search, we need to make sure that we have an instance of a cancelationTokenSource. We'll also set the Search button text to Cancel. This will make the UI a little bit more intuitive. If there's no cancelation and the asynchronous operation completed successfully, we'd like to set the text on the Search button back to the original text, as well as setting the cancelationTokenSource to null. If you don't, you'll get some really interesting bugs. I do urge you to play around with that yourself. This method is getting rather big. Let's just refactor a few parts of the method to a new one. I extracted the code that loads the file from disk and maps that to a stock price to a different method. This method now needs to be capable of canceling that operation. Let's add a parameter to the method, which will allow consumers of this method to pass a CancelationToken. As mentioned earlier, the CancelationToken is what is used to keep track of a cancelation. You might have already spotted this. Task.Run provides an overload that allows you to pass a cancelationToken. Let's use the cancelationToken that we've passed into the method. In Search_Click where we call this method, we also need to make sure that we pass the cancelationToken. The first time the click event handler is called, it will create the new cancelationTokenSource with a brand‑new cancelationToken. The second time it's triggered, there will already be a cancelationToken. It will then signal to the previously created cancelationTokenSource that it needs to cancel. What do you expect this to do? Let's run the application and see how it behaves. Search for a stock and then quickly hit Cancel. Why didn't this cancel the search? The cancelationToken passed to Task.Run does not automatically terminate the operation. In fact, it's only relevant when starting the operation. If the cancelation was requested before Task.Run executes, it would know that the cancelation had been requested, and therefore it wouldn't run the asynchronous operation. Then, how do you properly cancel the operation? The problem is, how would your code know where and when to cancel, and really, what to do when you've requested a cancelation? It won't just magically be canceled because you've passed the cancelationToken and signaled for a cancelation. You need to handle the cancelation as well. Given that we've already refactored the code for loading each line from disk asynchronously, this gives us the opportunity to just return the amount of lines that we've already loaded when there's a cancelation. This means it's handled gracefully. In the iteration where we load each line asynchronously, we can check if a cancelation has been requested. If a cancelation has been requested, we want to do something about that. Using the cancelationToken variable inside the anonymous method means that we capture this variable, and it's available throughout the asynchronous operation. The cancelationToken provides a property to check if a cancelation has been requested through the cancelationTokenSource. We could also call the method, ThrowIfCancelationRequested, which would throw an exception if it's been requested to be canceled. Calling this would throw an OperationCanceledException. Doing so will mean that the exception is captured in the task, it's set to a faulted state, and you can then properly handle that inside your continuation. To handle our scenario better, we're going to check if the cancelation is requested. If that happens, we'll just return the amount of lines that have been processed so far. When we return that from the task, its continuation will execute, as long as we haven't specified a task continuation option to say otherwise. You can pass the CancelationToken to the ContinueWith method as well. The same principle applies here. It won't magically be canceled when the cancelation is requested. You could introduce a continuation that only runs when a task is canceled. A word of advice: it won't run a continuation with the task continuation options set to only on run to completion if the task is marked as canceled. Run the application and let's search for a stock. We requested a cancelation. It canceled the operation of loading more lines, then continued to process the lines, then populates the data grid with the stocks that it had time to load. You could now experiment with introducing the cancelation for the continuations as well. If you need to specify both the TaskContinuationOptions, as well as the cancelationToken, you also have to set another parameter. This parameter is specifying the task scheduler. To keep it simple, you can use TaskScheduler.Current and this will use the TaskScheduler that is associated with the Current context. Finally, there's one more thing I want to add. I want to add a note in the application whenever a cancelation has been requested, although I don't want to associate that functionality with our task and simply append a continuation. I'd rather use an alternative approach. I'd like to request the cancelationToken to let us know when there's a cancelation. The cancelationToken allows you to register a delegate that will be called when the cancelationToken is canceled. This will execute on our context. So we can invoke the UI from this anonymous method. In this action, let's just update the UI to say that a cancelation was requested. As you now search for something in the application and immediately request the cancelation, we can see that this worked. It now added a note to tell us that there was a cancelation. This shows how easy it is for us to introduce a cancelation by using the cancelationTokenSource together with a cancelationToken. This provides functionality to request a cancelation. We also got an understanding of the fact that we need to handle the cancellations gracefully, and that it won't just automatically terminate an operation. I'd like to give a word of advice on how you chain your continuations. Have a look at this example. Each continuation we see here is registered with the task at the top. If the CancelationToken is set to canceled, the task passed into the continuation will notify you that it's been marked as canceled, which makes a lot of sense. If we change this code and chain the continuations together, meaning that we'd add the continuations to the task that is returned from ContinueWith, we'd get a little bit of a different result. This is because only the first continuation has a reference to the original task. The subsequent continuations reference the task of the previous continuation and that have not been marked as canceled. This is something that could potentially introduce bugs, so keep this in mind.

Cancellation with HttpClient
Some asynchronous libraries, like the HttpClient with its GetAsync and all the other asynchronous methods on that, will handle the cancelation without you having to do much more than simply passing the cancelation token. Depending on which library you use, the result of canceling an operation may differ. Sometimes, it might return partial data, and sometimes it throws an exception to allow you to know that the task has been canceled. There is a file in the solution called StockService. This class exposes a method called GetStockPrices. This one takes two parameters. The first one is the stock identifier, the second one is the cancelation token. Internally, this method uses the HttpClient, like we've seen previously. This performs a request to fetch the stock prices from the API, then returns the deserialized stock prices back to the caller as a list. You can see that it's using GetAsync. This gets the cancelation token passed to it. This call is awaited, and if the application triggers a cancelation by requesting the cancelation token source to cancel, this will immediately stop the web request and throw a TaskCancelException. Let's go ahead and change our code to use the StockService. Instead of loading all the data from disk, we don't have to do too many changes to the application to make this work. We can remove the call to search for stocks, as well as its continuations that we introduced previously. Then replace this with using the StockService. We can then call GetStockPricesFor together with the StockIdentifier, as well as the cancelationToken. This time, we're going to use the async and await keywords again. In the continuation, we can now make sure that we update the UI accordingly. Using the async and await keywords again is in preparation for what's going to come in the next module. If a cancelation is now requested in the application, the call to our search for stocks method will throw an exception. The await keyword will identify this; therefore, we need to make sure that this is properly handled in our try and catch blocks. Like earlier, we'll make sure that a note is added to the application. When there's an exception at the end of the method, we need to make sure that we reset the UI as well as the cancelation token source. Let's run the application and make sure that we didn't break anything. We can still search for stocks, and we can cancel the operation. Without having to handle the cancelation ourselves, we've now seen how this is handled with the HttpClient. This will make it easy for us to introduce cancelations when we consume resources. We saw that it's easy for us to introduce the cancelation token no matter if you're using Task.Run, when creating your own asynchronous methods, or when using things like the HttpClient.

Summary
This module has been packed with information about how you can approach your asynchronous programming, in particular, when using the Task Parallel Library, as well as when combining this with the async and await keywords. One of the most important takeaways from this module is to remember that the continuations are executed on a different thread. For a continuation that executes on a different thread other than the one spawning the operation, you need a way to cross‑communicate over different threads, in case you want to update the UI. Otherwise, you'll end up having issues and getting exceptions, and that's not really what we want. The task is a simple reference to an asynchronous operation. We can wrap pretty much anything in a Task.Run. The work passed to Task.Run is then scheduled to execute on a different thread. This means that we could take any code piece that we have and ensure that this is going to be executed on a different context. Do remember that the code that you wrap inside Task.Run could, of course, use something like the dispatcher to communicate back with the UI, and I'd recommend that you make sure that the code that you pass into Task.Run doesn't, in fact, block your application. You avoid that by doing as little work as possible in what's passed to a dispatcher. It also means that you're not forcing any asynchronous operation to run on the original thread. A very common pattern is to provide two alternative methods, one method that is a synchronous, and the other one is asynchronous. Make sure that you don't simply wrap the synchronous method call in your asynchronous method, as this would be bad practice and could cause problems like we talked about earlier. A better approach is to copy the code from the synchronous method over to your asynchronous version and make sure that all the code inside your new asynchronous method is in fact not blocking your application. We can tell the continuation to execute only when it succeeded; if there's a failure, or if it's been canceled. This means that we can make sure that we validate the success of our tasks and notify users accordingly. To summarize everything that we've talked about in this module, first, we talked about introducing the task in the application. We saw how we can wrap synchronous code in a task to offload the work to a different thread, really to run that somewhere else. In our case, that was to relieve the UI from too much work. Then we explored how to get the result, or potential exceptions, from a task by introducing a continuation. This gives us the capability to indicate that the continuation should be executed only when the task ran to completion. You could also say that it should only run if there was an exception or cancelation. All of this combined gives you a really flexible application, which allows for starting a lot of asynchronous operations and waiting for them to complete before proceeding to do other work. The continuations provide a way to get notified when that particular work has completed. So you can handle a result. You have now seen how you can combine using the async and await keywords together with the task from the Task Parallel Library. This allows you to introduce your own asynchronous operations. Last, but not least, we've discussed the difference between ContinueWith and using the await keyword. In fact, ContinueWith will execute the continuation on a different context, while the await keyword will allow you to run the continuation on the original context. Now, you know how to build really powerful applications that no longer block the UI or main thread. You are now capable of making the best use of the asynchronous principles by introducing a task for things that doesn't already expose asynchronous operations. You can couple this with the async and await keywords, and with that, you'll have an even more powerful application that gives your users a really great experience. Next up, we'll explore the Task Parallel Library even further, to learn how to apply its extremely useful methods to build even better performing applications.

Exploring Useful Methods in the Task Parallel Library
Exploring the Task Parallel Library
Welcome back to the course, Asynchronous Programming in C#. I'm Filip Ekberg, and now we are going to further explore how to use the Task Parallel Library. The task provides a whole lot of useful functionality. We've seen how to use it together with the async and await keywords, as well as on its own. Throughout this module, you will learn about: how to know if all the tasks in a collection of tasks have all completed, how to run a continuation when at least one of the tasks in a collection of tasks has completed, starting multiple tasks and process the result as it arrives, creating a task with a pre‑computed result, and finally, you'll learn more about the execution context and controlling the continuation. All these topics are important when working with asynchronous programming in C#.

Knowing When All or Any Task Completes
I'd like the users to be able to search for multiple different stocks at the same time. This means starting multiple asynchronous operations simultaneously. We'll eventually need a way to know if all of it succeeded before we can add the result to the application. This means that I want to be able to call GetStocksFor on the service class multiple times, each call with a different stock identifier. Let's assume that the user enters the different stocks separated by a space or a comma. We can split the input and get a list of the different stocks that we'd like to search for. Now we want to search for these using the service. Right now, we're kind of facing a problem. We don't want to proceed updating the UI until all the data has been loaded from the service. This means loading all the stocks. The way that we're going to do this is by loading all of these different stocks in parallel and then wait for these asynchronous operations to complete before proceeding to updating the UI. Let's iterate through the list of stock identifiers, and for each identifier, we're going to call GetStockPricesFor. This will start an asynchronous operation, and the method, as you know, returns a task that is representing this asynchronous operation. We'll store this task. This will help us to keep track of all the asynchronous operations that we've started in this loop. Let's call this collection of tasks, loadingTasks. This will now hold a list of tasks that will in its turn, when they're done, return an IEnumerable of StockPrice. If we were to await each call to GetStockPricesFor, that would mean they would execute one by one. We want them to run in parallel, therefore we won't add the await keyword at this point. Instead, we'll capture the task, which is a reference to the ongoing asynchronous operation. If we were to search for 10 different stocks, this will create 10 different tasks that will pretty much run at the same time and invoke the API asynchronously. With this list of tasks, how do we make sure that all of these completed before proceeding updating the UI? Ideally, we'd want an aggregated result and then populate the UI with a flat list of all the stock prices. Fortunately for us, there was a static method on the Task class that will help us achieve this. The method WhenAll will allow us to pass an IEnumerable, or a list of tasks. Using Task.WhenAll will create a new task, which will be marked as completed only when all the tasks in the list that we pass to the method are all marked as completed. Task.WhenAll is an asynchronous operation. You know this because it returns a task, so you probably know what to do next. You will have to await it. The result of Task.WhenAll is, in fact, a list of all the results produced by the asynchronous operations. We can store this in a local variable that we're going to call allStocks. Since the result of each asynchronous operation is an IEnumerable of StockPrice, the result of Task.WhenAll will be an array of IEnumerable of StockPrice. For us to concatenate or flatten this list, we can use a link method called SelectMany. Simply add allStocks.SelectMany, and this will take all the different lists and put them in one big list of stock prices. We can now search for two different stocks by separating them by a space. When the data is loaded, we can see that the list is populated with the stock prices for both of these different stock identifiers. This here shows you how you can start off a lot of asynchronous operations and then use Task.WhenAll to make sure that you don't proceed in your method until all those different operations are completed. Now, I want to make sure that our operation completes within a given time. I'm going to introduce a variable using Task.Delay, which will allow us to get a notification when the particular time has passed. This creates a task, which will simulate that it takes the given amount of time to execute. We can now use this task produced by the Delay method just like any normal task produced by an asynchronous operation. The idea here is that if it takes longer than a few seconds, we're going to cancel all the ongoing operations. You're probably wondering now how you would do that. First, let's store the task produced by Task.WhenAll in a separate task. Since we no longer use the await keyword, this doesn't produce the result of all the stocks. Instead, it's a representation of the asynchronous operation that is loading all the stocks. The idea is now to ask whichever of these two tasks completed first. To do that, we can use Task.WhenAny. This will give us a notification when one of the tasks that are passed to it completes. This method returns a task of a task. This means it creates an asynchronous operation, which result will be the first task that completes from the list that is passed to it. If the delay task is first, that one will be returned, and if the loading stocks task completes first, that will be the one returned. Let's await the task. The resulting variable will now contain a reference to the task that completed. We can compare this against the delay task. If they're the same, we're going to go ahead and cancel all the ongoing operations by calling cancellationTokenSource.Cancel. Every task that uses the cancellationToken will now be signaled to cancel. Then let's throw an exception just to make sure that the flow of the application works properly. If it wasn't the delay task that completed first, it means we have all the stocks available. We can then grab all the stocks from the Result property. Remember, it's completely okay to use the Result property here because it's been properly awaited before. Run the application and search for a few stocks. You can see that it terminated the operation because the delay task completed first. We can see in the Notes section that we got a cancelation requested. You can experiment with the delay and see how the application behaves. Obviously, a few milliseconds or a few seconds might be a little bit too short. I'm going to change this to 120 seconds just to avoid any problems when running the application. An alternative approach to this would have been to simply use the cancelationTokenSource to indicate that we want to cancel after a certain given amount of time, but this illustrates that we can initiate a lot of asynchronous operations that run in parallel, which we can then await as a chunk of operations that all need to complete before we proceed. We can also indicate that we want to wait for any of them to finish and then continue as soon as that is available. This gives us a lot of flexibility. I'd suggest that you experiment with the alternative approach of canceling the operations on cancelationTokenSource. You now also know how to use the async and await keywords together with Task.WhenAll and Task.WhenAny to understand how to build really powerful applications.

Precomputed Results of a Task
Have you ever been in a situation where you'd just like to have a task with a predefined result, without introducing an asynchronous process? This is where pre‑computed results are really handy. Currently, every time we search for a stock, the application goes ahead and calls the API. During the development process, this might be unnecessary. Consider that sometimes querying an API will actually cost money. In other situations, you may want to write tests for the code that consumes the API without having to query the API each time you run the test. In those situations, it's common to set up a fake class, or what's known as a mock object. The only purpose of this class is to provide the same functionality as the real service. The class will provide the same method signatures, and instead of querying the API, you may just return some fake and predefined data. Instead of using the stock service, we're going to introduce a mock object. The mock object and the real stock service will both share the same interface. This means they'll have the same contract. If we use the interface throughout the application, the code won't have to care about if it's the real service or the mock object. The MockStockService will implement the interface and have a few predefined stocks that all live in memory. This will just make it easier for us to run the application and try a few things out without always querying the API. To implement the interface, we must implement its methods. The method signature specifies that we must return a task of an IEnumerable that contains the stock prices. This makes sense because the consumer of the method expects this to be asynchronous. Since we are creating all the sample data in an in‑memory list of stock prices, there's no real reason for us to run a task and add the async and await keywords. In these situations, we want to simply return the object wrapped in a task without running anything asynchronously. There are a few ways for us to mimic the behavior of a result retrieved through an asynchronous operation without running anything asynchronously. The Task class comes with some useful functionality. For instance, you can construct a task that's already marked as completed by using the static property, CompletedTask, on the Task class. You normally do this when you override a method or implement an interface that requires you to return a task, but your method doesn't run anything asynchronously, therefore you don't care about starting a new task, nor do you want to mark the method with the async keyword. You'd simply want to indicate to the caller that everything completed successfully without changing the method signature. There are also other static methods on the task to construct a specific task for different scenarios. You could construct a task that is marked as Canceled, or a task with an Exception, all of them without running anything asynchronously. We'd like to return a list of stock prices as a result on a task without running a new task or adding the async and await keywords. Adding the async and await keywords in this situation is a bit unnecessary, as it introduces things like a state machine, which just adds a lot of overhead that we don't need. Instead, we'll use the method Task.FromResult and pass the filtered list of stock prices as the parameter. This creates a task, which is marked as completed with a result of the searched stock prices stored inside of it. This allows us to represent a completed task with a result, without running anything asynchronously. We've now introduced a mocked version of the stock service. You can now replace the stock service with the mock stock service, and for anywhere where the IStockService was used, you could simply pass an instance of the MockStockService. Consuming this mocked service is identical to the real one, as they both implement the same interface. When you search for the stocks available in the in‑memory collection, you can see that you get the mocked data, and it only took a few milliseconds. That certainly indicates that it used the in‑memory data instead of querying the API, which was a lot slower. This is a great addition and a great way for us to work with our asynchronous libraries in testing, or in situations where we don't really want to introduce any asynchronous principles. Speaking of testing, let's add a test project to the solution and write a test against the MockStockService. This is just to show you how you can use asynchronous principles in test projects as well. Go ahead and add a new project. You can choose to add an MSTest project. Let's call this StockAnalyzer.Tests. We can now write a test class for the MockStockService. Normally, you write a test for methods that use the interface IStockService, and pass the MockStockService into that, but I'd like to focus on using asynchronous tests. Therefore, we'll just keep it simple. Let's create a test that checks that we can load all the Microsoft stocks through the stock service. We'll create an instance of the mocked stock service. We can now query the in‑memory stock service. As this returns a task, we can use the async and await keywords. Let's also add a few asserts to make sure that we grab the correct amount of stocks from this mocked service. This would also be a good point to assert other things that might be relevant to the application. We can now run the test and make sure that it works properly. This shows that we can use async and await, even when we are writing tests. This is extremely beneficial, as a lot of the code that we write nowadays is going to be asynchronous, and being able to test our code is crucial to the success of our applications. Using pre‑computed results with the tasks will certainly help us achieve this.

Process Tasks as They Complete
The application is capable of searching for multiple different stocks at the same time. It waits for them all to be loaded before populating the UI with the information. I'm sure that we can improve the user experience, and instead of waiting for all of them to finish, we'll add the data for each stock as it arrives. Let's change the MockStockService back to the StockService. We are already querying the API for the different stock identifiers that we are searching for, and we're doing that simultaneously. There are a few changes needed to populate the data grid with the data as it arrives to the application. First, we'll need a collection, or a list where we can dump all the data that we're getting back from the API in the asynchronous operations. For this, we'll use a ConcurrentBag. This is a thread‑safe collection that allows you to add items from multiple different threads without worrying about data being lost on the way. If you use a normal list, you might end up having problems because you might end up trying to add data from different threads at the same time. Therefore make sure that you're using a thread‑safe collection. Next, we'll go down to the loop that starts a task that calls the API for each stock identifier. This foreach loop will make sure that we start all of these queries to retrieve the stocks, pretty much at the same time. What we're also going to do now is to chain on a continuation that will run when each API call completes. Performing this API call might be rather quick. To make it a little bit more interesting, I've already added a short delay in the stock service. You'll see the Task.Delay that will simulate that each time we call this method, it's going to incrementally take a little bit longer. Obviously, this isn't something you'd want to ship into production, but it makes it a little bit more interesting when we are running our applications to learn about all of these patterns and principles, as it better illustrates the benefits of the changes we are about to make to the application. Each service call now has a continuation chained onto the task that represents the asynchronous operation. Inside the continuation, we are going to add the data from the result to the thread‑safe collection. Let's just pick the five first stocks that were returned from the API. This result is available on the task that was passed into the continuation. You can grab it through the Result property. Notice that there's an error. Let's return the five stock prices that we filtered out. Now, the code further down still works and compiles. These items now need to be added to the ConcurrentBag. This requires us to add each item, one by one, so we'll increase a foreach loop and add each of them independently. This is starting to look great. For each company or stock identifier, we perform an API call. We'll then take the five first stocks and add that to the bag of stock prices. All that's left now is to update the UI. Previously, this was done a bit further down at the bottom of this method, but we're going to remove that, because we are no longer waiting for all the API calls to finish. We'd now like to update the UI each time we're inside the continuation, because that's when we get new data. As always, when invoking the UI from an asynchronous operation, you'll have to use the dispatcher or equivalent in the UI framework that you are using. The work posted to the UI thread will ask the data grid to be updated with a copy of what's currently inside the ConcurrentBag. This will take all the items that are currently available in this ConcurrentBag and create a new array of those items, which will then be presented in the data grid. Before we run the application, if you haven't changed the timeout to more than 2 seconds, now it's probably a good idea to do that. Otherwise, you will probably end up having issues. Optionally, you can remove the timeout handling completely. If you do, make sure to await the allStocksLoadingTask, which is simply a reference to the task created by Task.WhenAll. We can search for Microsoft, Google, and a few more that we know exist in the collection of stock prices. The application is populated with Microsoft, Google, and all the other stock data as it arrives from the stock service. This here shows you the power of chaining on continuations onto multiple different tasks. This allows us to handle the result as it comes back to the application.

Execution Context and Controlling the Continuation
Sometimes you might not really care about coming back to the original context when running the code in the continuation. Marshaling back to the original context can in some situations be considered a rather expensive operation. In those cases, wouldn't it be very handy if we were able to configure the awaiter and let the await keyword know that it doesn't have to marshal back to the original context. There's definitely a way for us to do that. Task provides a method called ConfigureAwait. This is used to let the awaiter know if we want to come back to the captured context in the continuation. To illustrate how this works, we're going to remove all of the code in the Click event handler. Then we'll introduce a new asynchronous method named GetStocksFor. This method will expect the stock identifier as its first parameter. It will use the StockService to call the API. After this call has completed and we've awaited the result, we'll add a note to the UI to indicate that the stocks have been loaded. As you should know by now, you can update the UI after the await keyword because the continuation runs on the original context. Since we're going to call this method from the Click event handler, we know that the continuation will execute on the UI thread. Finally, this method will return 5 of the stocks back to the caller. Let's call and await this method in the Click event handler. What do you reckon happens if we configure the awaiter for the Stocks loading task and indicate to the await keyword that we don't care about coming back to the original context, which would be the UI thread. ConfigureAwait allows us to specify if it should continue executing on the captured context. This defaults to true, which executes the continuation on the same thread that started the asynchronous operation. If you guessed that this would cause a problem, you absolutely guessed correct, because now we're doing cross‑threading. As you see, it didn't work and the UI was not updated. What happens here is that when we tried to update the UI, we were no longer on the original context, which means that we are no longer on the UI thread. This is rather interesting. Using ConfigureAwait impacted what happens further down in the continuation of that method. But what happens if we move the UI interaction to the Click event handler to after the await keyword where we are back from GetStocksFor. What's really interesting is that the application now works. The note was added to the application after this asynchronous operation completed. What this tells us is that using ConfigureAwait only affects the continuation in the method that you're operating in. This means that the continuation in GetStocksFor executed on a separate thread, while the continuation inserts Click marshaled back to the UI thread. When we are inside the Search_Click event handler, we're in fact in a different asynchronous context. Each method marked with the async keyword will have its own asynchronous context. If you're also configuring the awaiter inside Search_Click to not marshall back to the captured context when we're back from this GetStocksFor call, we'll have the same problem as we had earlier. This is now trying to perform a cross‑threading call. ConfigureAwait is extremely powerful, as it allows us to continue the execution without marshaling back to the original context. This means we could see an improvement in the performance of the application, as it doesn't have to wait for the original context to be available. This also means that in these cases there shouldn't be anything in the code of your continuation that relies on being back on the captured context. In this case, the continuation shouldn't depend on being able to update the UI. Another important note is that thread static values from the original context won't be available in the continuation. This was a common problem in traditional ASP.NET when trying to access the HTTP context. Nowadays, that's more rare. In previous versions of ASP.NET, pretty much everyone was told to use ConfigureAwait for all their asynchronous operations. If you're working in ASP.NET Core or newer, using ConfigureAwait is totally useless. There's no such thing as going back to the captured context. ASP.NET Core no longer have what's called a synchronization context, therefore, ConfigureAwait doesn't have an effect at all in ASP.NET Core. So if you're working in ASP.NET Core or newer, you can completely get rid of that. With that being said, when is it appropriate to use ConfigureAwait(false)? Whenever you build libraries. If you're a library developer, you won't necessarily know if the people that consume your libraries will be building WPF WinForms, ASP.NET 4.8, ASP.NET Core, Xamarin or other types of .NET applications. That's when you use ConfigureAwait(false), because in your library methods in your continuations, you really shouldn't care about the original context. Using this inside your libraries will work the best across all different types of .NET applications. If you're building a library that exposes asynchronous methods, those probably shouldn't try and force themselves back on the original context or depend on thread static values. Be careful about using ConfigureAwait(false), because you really need to be able to guarantee that you don't need to access things that live on the original context when you're back in the continuation.

Summary
This module has given you a better understanding of how to best use the Task Parallel Library to construct powerful operations. As you further explored the methods provided by the Task Parallel Library, you learned about the continuation, how to configure it, and combined all of this knowledge to build something that ultimately gives your users a better experience. You learned how to start multiple asynchronous operations that execute in parallel. This allows the application to load data simultaneously, which increases the performance of the application. You introduced the method Task.WhenAll to add a continuation that only executes once all of the task completed. This provides a task that will be marked as completed when all of its referenced operations are done. We also introduced Task.WhenAny, which provides a way to retrieve the task that completes first in a list of referenced operations. Then you saw how to introduce a pre‑computed result. We can construct a task without running anything asynchronously with a pre‑defined result on that task, this by using Task.FromResult, which is often something you'd use in situations where the interface you're implementing requires a task to be returned. But you might already have a static value that you want to return without having to run anything asynchronously. You could also construct a task that is marked as canceled or faulted. This would be appropriate in, for instance, tests. We introduced Task.FromResult in a mock object that allows us to work with an in‑memory collection. This ensured that we didn't have to call the backend API each time we ran the application or a test. This clearly speeds up the development process, and for the situations where the API invocation might cost money, this is a really good alternative. You then introduced a way to process tasks as they complete. This allows you to start multiple tasks that run in parallel. As soon as a task completes, you'll have a continuation on that task to be able to handle the result for that operation. With this, we explored a way to aggregate the result of all the operations using a ConcurrentBag. This thread‑safe approach lets us populate the application with information as soon as it's available. Finally, we continued to talking about the continuation. You added ConfigureAwait to tell the awaiter to avoid marshaling back to the original thread. Doing so allows the code to execute faster, because it's not required to wait for the original thread to be available. You found that ConfigureAwait only affects the current asynchronous method where it's being used, as well as the fact that it's no longer necessary in ASP.NET Core and later. By now, you've got a good idea of how to introduce a task, the async and await keywords, controlling and introducing a continuation, as well as how to follow some patterns to build better performing applications. You are now ready to start looking at some more advanced topics. Continue to the next module, which will take your asynchronous programming to the next level.

Async & Await Advanced Topics
Async and Await Advanced Topics
Welcome back to the course, Asynchronous Programming in C#. I'm Filip Ekberg, and now we are going to further explore async and await. This module is going to teach you about some advanced topics and best practices to help you build more robust applications, as well as more readable and maintainable code. You will learn about important patterns such as asynchronous streams and asynchronous disposables. After this module, you will have a good understanding of when a deadlock occurs, how to prevent it, and ultimately why it occurs at all. You will learn this by exploring what is generated by introducing the async and await keywords. Understanding the implication these keywords have on your generated code is important. Not only is it interesting to know, but having a deeper understanding of how the code is affected will make you a better developer, as well as providing you with the information to write better performing code. Continue through this module to learn all about these important topics.

Asynchronous Streams and Disposables
Asynchronous streams allow us to work with streams of data, no matter if the data is coming from the web, a database or disk or any other type of asynchronous operation. The idea is that we can process items as they are arriving to the application in a better and more intuitive manner. Let's jump into Visual Studio and have a look at what this looks like. Asynchronous streams requires the project to target .NET Core 3.0 or later. What we'd like to do is present the stock information in the application as it arrives. We saw one way of doing this earlier, but it's time to introduce a more readable and maintainable alternative. This application will now work with the local file again to stream the stock information that it loads. Instead of having to wait for all the stocks to be loaded, I now want to display each line of the stock price as the application gets ahold of that data. Let's start off by introducing an interface that will allow us to introduce different implementations of this stock streaming service. I've already created this new file called StockStreamService and it's available in the exercise files. inside this file, there is an interface called the IStockStreamService. This defines our contract. This contract tells us that we need to implement a method called GetAllStockPrices. GetAllStockPrices returns an IAsyncEnumerable of StockPrice. This is new and not something that we've seen previously. The IAsyncEnumerable allows us to define that we now have an asynchronous enumeration. Whoever is consuming the IAsyncEnumerable can use it, retrieve and work with each item as they're available to process. Let's create a first implementation of this streaming service. We'll start off by creating a mocked version, which will allow us to understand how all of this works. We can now proceed to implement the method defined by the interface. The goal is to return a few different stock prices, and we want to fake that we've received these asynchronously. Mark the method as async so that we can introduce a few delays that simulates loading some data. You'll notice that we don't need to return a task. The compiler doesn't complain about the fact that the return type is an IAsyncEnumerable. We can use the async keyword together with the IAsyncEnumerable in order for us to indicate that we want to introduce an asynchronous enumeration. This now gives the method the capability to use the await keyword. We use it to simulate that it takes a little bit of time to load some data that is then returned back to the caller. Normally when we return a value we simply use the return keyword, but since we are now setting up an enumeration, we need to introduce something else as well. We need to add the yield keyword to indicate that we now have an item available to process. To make it a little bit more interesting, let's return a few values. Each StockPrice from this mocked service will take approximately 500 milliseconds to retrieve, and when it's available, will notify the iterator that it can now process the item. What's really interesting here is that we now use the async keyword, the await keyword, the IAsyncEnumerable, as well as the yield keyword in order for us to create an asynchronous stream of StockPrices. To summarize what this method achieves is that it fakes that it takes about 500ms to retrieve an item. Once that's retrieved, we then indicate to the iterator that we have an item available for it to process. Then it goes ahead and retrieves the next item, which also takes approximately 500ms, and then it continues doing the same for the rest of the items. This produces a stream of data. Now, all that we need is a way for us to use this in the application. Let's jump over to the MainWindow class and consume a stream of data. To use an asynchronous stream, the method that consumes it needs to be marked as async, as well as somewhere in the method use the await keyword. To dynamically add items to the UI as they arrive to the application, we are first going to introduce what's called an ObservableCollection. The data grid will then use this as its ItemsSource, and as items are added to it, they will appear in the UI. Now, how do we consume the stream of data? What we've done previously is using the await keyword in front of the asynchronous method that returns a task. In this case, it's a little bit different because we are now returning an enumeration that will asynchronously allow you to retrieve items. Instead of having the await keyword in front of the method call, we're going to use the await keyword in front of the foreach loop, where the foreach loop will then make sure that it's properly awaiting each item in the enumeration. We've now introduced a foreach loop, where each item in the collection will asynchronously be delivered to the foreach loop, which means that the body of the foreach loop is a continuation. We could now check if it's the stock we are looking for and then add this to our collection. Now that we run the application, you'll see that as soon as we retrieve an item, we're going to have that added to the data grid. The foreach loop will asynchronously retrieve each item. How about replacing the mocked implementation with something that uses the file system. As you process each line in this file, you want to return that to the caller. We can introduce a new class called StockDiskStreamService, which will implement the interface IStockStreamService. It's going to read each line in the stock file, process that, and then return it back to the caller. The caller can then take care of either processing the element or just request the next element to be loaded. The implementation of this method is going to be very similar to the one that we had earlier in the course. There are, of course, going to be a few small changes. First, open a stream to a file on disk and process each line asynchronously. Skip the first line because that's the header row, and then we're going to iterate over the rest of the lines. Let's make sure that we also check if a cancelation has been requested. If we requested to cancel this operation, we are simply going to break out of the loop and we can then just return back to the caller. It's best to handle this and cancel gracefully. Once we've got an instance of a StockPrice parsed from the comma‑separated values, we can use yield return to let the consumer of the asynchronous enumerable know that there is now a new item to process. This means the item will be returned to the foreach loop for it to process. We didn't, in fact, add any fake delays or anything like that to this method. What we're doing is scanning a potentially large file and returning an object back to the caller for it to process as soon as it's available and loaded into the application. We can now go back to Search_Click and change this to use the StockDiskStreamService. As you run the application and search for stocks, you'll notice that it fills up the UI as soon as the items are loaded and parsed. We didn't have to wait for the entire file to be processed before returning anything back to the application. Not only does this open a whole lot of different possibilities for us when building our applications, the code is also much more readable and easier to maintain. If we compare the current state of the application to what we had earlier in the course, the user experience is a lot better, and that's partially thanks to the asynchronous streams. In cases like this where we are scanning through files looking for certain records, using the asynchronous streams is beneficial. We can communicate back to the UI as soon as we have data available. That's going to provide a better user experience. The asynchronous streams allow you to asynchronously retrieve items. Consuming an asynchronous stream is as simple as adding the await keyword in front of a foreach loop. To produce an asynchronous stream, you use the async keyword together with an IAsyncEnumerable, the await keyword, as well as the yield return. This is a powerful addition to the language and provides a better way of expressing yourself, although without understanding tasks and the async and await keywords, this would have been a lot harder to grasp. C# also introduces the concept of an asynchronous disposable. If a class implements the interface, IAsyncDisposable, you can await the using statement that will dispose of the object. The syntax for this looks very similar to the asynchronous stream with the foreach loop. You simply add the await keyword in front of using. This can also be used together with using declarations. This may look odd if you haven't seen the using declarations before. This will ensure that the resource is disposed at the end of the method. As you then await it, the asynchronous disposable will also be validated. These two additions makes a great addition to asynchronous programming in C#.

The Implications of Async and Await
The time has come to talk about the implications of applying async and await in your applications. We're going to discover some of the code that is generated when you apply these keywords, and we'll talk about some of the problems that you may run into because of that. Understanding how async and await works internally, definitely makes it a lot easier for you to find potential problems. I've mentioned a few times that introducing the async keyword generates a bit of code and it introduces something known as a state machine. Understanding the implementation of the state machine isn't as important as understanding its purpose. The state machine is there to keep track of the tasks in your current asynchronous context. It will handle the execution of the continuation and a way to provide the potential result. It also ensures that the continuation executes on the correct context, which means that it also handles the context switching. Of course, in the cases of there being an exception, it's also in charge of reporting the errors from that asynchronous operation which you are awaiting. Let's look at some code to understand this better. Take this Search_Click event handler as an example. This is a very simple method. It hasn't yet introduced the async and await keywords. Let's compile the application and open it in a tool such as ILSpy. This is an open‑source project that allows you to decompile a .NET application. This provides a way to look at the IL that was generated by the compiler. The generated IL can be inspected as C# code. This is equivalent to what we see in Visual Studio. In fact, it looks identical. What do you think happens if we go back to Visual Studio and mark Search_Click as async without using the await keyword anywhere in the method and then recompile the application? We can see that we're getting a hint that we should probably not mark this method as async unless there's an await keyword in the method. While this is correct, let's leave it like this for now. Let's open the recompiled application and see if anything changed. Behind the scenes, what's going to happen here is rather interesting. Adding the async keyword actually had quite a big impact on the code that's generated. The Click event handler now turned into something completely different. This code here doesn't look anything like what we have in Visual Studio. It introduced something called a stateMachine. A stateMachine is generated for every method that you mark as async. Each method that you have marked as async will turn into something looking like this. This generated code uses something called the AsyncVoidMethodBuilder, which is something that it's using for methods that are marked as AsyncVoid. Then it's starting this process using the stateMachine. The stateMachine also takes care of things like running the continuation. How about we have a look at the stateMachine. And here we have it. This is the stateMachine that was generated for the Search_Click method. The stateMachine has a reference to the MainWindow class. There's a method called MoveNext. In fact, inside MoveNext is where everything interesting is going to happen. All the code that we've seen so far is, in fact, executing on the caller thread. This means that the entire state machine executes on the same thread as Search_Click is being executed on. Can you spot something familiar here? You can see that we have Notes.Text. So it took all the code from the Search_Click method and moved it into this generated method. It also put all of the code inside a try and catch block. This shows you why methods marked as async Task captures exceptions. If there's an exception, it's going to use what's called the builder to set the exception. It also uses the builder to set the result to indicate that it completed successfully. I'd say that this is a rather drastic change to the code of the application, all with just a simple async keyword. Another important aspect is that when it's using the AsyncVoidMethodBuilder and trying to set the exception, there's no task to set it on, which is why this would tear down the entire application. How about we add a little bit more interesting code to the application? I'm going to create a very simple method called Run, which is an asynchronous method marked with the async keyword and returns a Task. We'll introduce a simple Task that returns a string. The result of this operation will be stored in a local variable. As we await the result of the asynchronous operation, this operation, in this case, immediately returns the string Pluralsight. We can compile it and inspect the code in ILSpy. And the first thing that's interesting is that we now have two stateMachines. We have one for the Run method, as well as one for the Search_Click event handler. Let's inspect the Run method. The signature indicates that it returns a task just as we previously stated in our code. What this method actually returns is the stateMachine's builders Task. This is a reference to all the ongoing asynchronous operations that's currently going on. This Task is what will have the potential exception, result or a cancelation set on it. Again, this here changed the method quite drastically. In this case, we also used the await keyword. If we check inside MoveNext inside the StateMachine generated for the Run method, it's going to look a little bit different. The StateMachine keeps track of the current state and has an awaiter to keep track of the current ongoing operation. To know if it's completed, while you don't have to understand the implementation in detail, this really just shows the complexity of introducing async and await. To further understand the complexity, let's add some code to the continuation of the Run method. Where do you think this code will live inside the generated stateMachine? At the bottom of MoveNext that was generated for this asynchronous method, we can see the code that executes the continuation. Again, this shows you that what's generated here is rather complex. It's doing a lot of heavy lifting for us, all to make it easier for us to introduce asynchronous principles in our applications. By applying the async and await keywords, it makes it a lot easier for us to work with these asynchronous principles. The heavy lifting is done by the compiler, and really it shouldn't be taken lightly. There are a lot of really clever things going on here.

Reducing the Amount of State Machines
We don't always have to worry too much about the generated code, but in really high‑performing situations, it may be necessary to keep the amount of generated code to a minimum. It may also reduce the potential occurrences of any errors. This code here is going to prove an interesting point. In this case here, we have a chain of method calls. First we have this method called Run, which calls Compute, that calls Load, and then when Load has executed all of its processing, it returns the result from the asynchronous operation. All the methods will use the async and await keywords to validate that the method it's calling completed successfully. Compute will await Load. Once that is done, it will return the value back to Run. Run also awaits that asynchronous operation before it returns a result. If we compile this and look at the generated code, it introduces a stateMachine for all of the different methods, one for Run, Load, and one for Compute. The stateMachine, as we saw, introduces complexity, and given this scenario, do we really need to introduce a stateMachine for each of these different operations? We've learned that we could simply await an asynchronous operation, both for the purpose of retrieving the value it returns, but also for validating its success. There is a way to refactor this method chain to make it a little bit better. Notice there's nothing happening in any continuation. They were all just returning a value back to the caller. If we instead remove the async and await keywords and return the Task, we'd end up having the same method signature. The application compiles fine, but suddenly we reduced the complexity and less code is generated. If we do this, it's extremely important that whoever is calling Run should be the one introducing the await. You must make sure that the asynchronous operation is at least awaited at the top level. If we were to call this in Search_Click, that would be the method which awaits this operation, so even though it's simple for us to introduce the async and await keywords. To work with our asynchronous operations more easily, we should be very cautious and we shouldn't take these keywords lightly. Think about the situations that you're working in and if there is a way for you to improve the performance by allowing the caller to take care of the asynchronous operation, do that instead. This would potentially allow you to reduce the amount of complexity generated when you're compiling your code. With a better understanding of the state machine and the implications of adding async and awaits, it's now time to proceed to discuss why a deadlock occurs and how we can prevent that.

Deadlocking
Have you ever wished to make your applications completely useless? Probably not. That would be a rather terrible goal. Sometimes it happens by mistake. A common problem with parallel and asynchronous programming is deadlocking. A deadlock, as you might imagine, is a really terrible situation to end up in. As an example, let's say that you have the UI thread. The UI thread is locked because it's waiting for some operation to complete. That operation cannot complete unless it can communicate back to the UI thread. That sounds really odd. It sounds like a really bad idea. Although ending up in this scenario is easier than it sounds, we need to understand why this problem can occur and how to avoid it. This is an asynchronous method. It's marked with the async keyword. The method returns a reference to the ongoing asynchronous operation. In the best of worlds, we'd simply await this call by using async and await inside the caller method, which in our case is going to be Search_Click. However, for some unknown reason, I really want this to run synchronously. That's why I'm going to force this to block until we have a result. I'm therefore going to call Wait on the Task that's returned from SearchForStocks. The Task provides a method called Wait, which will block the current thread until the data for the task is available. It won't proceed executing the code beneath of it until this asynchronous operation completes. Running this code in fact results in a deadlock. This change was rather terrible. It proves the point that it's super easy for us to get a deadlock, especially when calling methods where we've marked them with the async keyword. But how come this introduced a deadlock? Given the fact that we know what happens when we introduce the async and await keywords, can you figure out why this is deadlocking? The reason it's deadlocking is because the state machine and the code inside of it is being executed on the same context. This means the state machine is being executed on the UI thread because the call originates from the UI thread in the Search_Click method. If that's the case, this mean that we are blocking the UI thread so it cannot proceed to execute the state machine, which means that the asynchronous operation cannot communicate back to the state machine when complete. To illustrate this in a slightly different way, have a look at the following scenario. We create this asynchronous operation. The asynchronous operation needs to use the Dispatcher or the synchronization context to communicate back with the original context. Now let's add a Wait to indicate that we want to lock the thread until all the processing has completed. But this process can never complete, because it cannot communicate back to the original context, hence, we have a deadlock. This is why we should really avoid forcing something to block when calling result or wait on an asynchronous operation that hasn't completed. In the situations where you really want to do this, let me show you a little bit of a hack that you can use to make this work the way you probably expected this to work in the first place. What happens if we move the execution of the state machine to a separate thread? What we're doing now is that we're taking the state machine for SearchForStocks and we're executing that on a completely different context. Then we block the UI thread until that thread completes, but we're not blocking the particular state machine from executing. So what do you reckon is going to happen now? It didn't block our current context, we didn't get a deadlock, so that's a slight improvement. Although, if SearchForStock depended on being on the UI thread in its continuation, you will now end up getting exceptions, so you may fix one problem, but introduce another one. The point here that I'm trying to show is how easy it is for you to get a deadlock, and it's mainly because we might forget about the fact that the state machine runs on the original context, and when we are trying to block that original context by using wait or the result property, that will lock our original context, so there's no way for the state machine to be marking this task as completed. The most important thing to remember is to avoid blocking your asynchronous operations, because that kind of makes the asynchronous operations pointless.

Summary
You've seen how powerful the async and await keywords are throughout this entire course, with a closer look at some of the advanced features and topics. You now have even more options when you're building asynchronous code. You can introduce asynchronous streams and asynchronous disposables. These allow you to write easier to read, maintain, and debug code. We explored the internals of async and await. This gives you a good understanding of how it affects the application. With this insight, it became rather obvious why a deadlock occurs and how to go about fixing that and preventing it from happening. This is something you'd want to really avoid. Most of the cases when it happens is when you block an asynchronous operation by either calling the result property or the Wait method. As a final step to master asynchronous programming in C# and in .NET, continue to the next module to learn more about the advanced scenarios and topics with the Task Parallel Library.

Asynchronous Programming Advanced Topics
Advanced Topics
Welcome back to the course Asynchronous Programming in C#. I'm Filip Ekberg, and this module will teach you more about some useful and advanced topics in relation to the task and asynchronous principles. After completing this module, you will know how to properly report the progress of a task and why that might be considered non‑trivial. You will also understand when it's needed to introduce what is known as a TaskCompletionSource. This is something that you'd introduce when you have a parallel operation which is not using the Task Parallel Library for its execution, but you'd like to consume that as if it were a task. We will then dig further into the Task Parallel Library and talk about attached, as well as detached tasks. This is for situations where a task might spawn a new task from within itself, and you'd like to have total control of how your parent task behaves.

Report on the Progress of a Task
You know the progress bar that we have in the application? Wouldn't it be great if we could update that based on the status of the asynchronous operation? I'm sorry to say, but the task in the Task Parallel Library doesn't support reporting on the progress. This means you'll need to build something yourself to report the progress of a task. Thinking about this problem, it's kind of obvious that you'd need to build something yourself. How would the framework know the progress of a task? How would it know what inside the asynchronous operation determines the progress? As an example, in the case of working with the stock file, would it update the progress based on how much of the file had been loaded or how much of the data that we've processed? Either way, we'd need to introduce our own way of computing the progress. Let's try and build something that reports the progress and updates the progress bar based on how many stocks have been completely loaded. This means that if we are searching for five stocks, we'll update the progress bar five times. In WPF, we were using a progress bar. If you're working in other applications, you might have similar controls available. Remember, this is just a UI layer. The framework component that we'll introduce works in any type of .NET application. What we're initially doing when we are clicking Search is that we are resetting the progress bar to some initial value. This is just WPF‑specific. We'll set the value of the progress bar to 0, which means we currently have no progress. Then we'll set the maximum amount of progress to the number of StockIdentifiers that we are currently trying to search for. To report the progress, we'll introduce something called Progress<T>. This class is available in any type of .NET application and it doesn't even require you to have a UI. Progress<T> provides an implementation of the interface IProgress<T>. This allows you to subscribe to an event, which will be triggered to notify that the progress is updated. Whenever there's an update to the progress, the event is triggered and gets data passed to it. The type of data is determined by the generic parameter on the progress. We could ask for the entire IEnumerable<StockPrices> that we've just loaded. When we are creating the instance of the class Progress<T>, it's going to capture the SynchronizationContext. The SynchronizationContext isn't something that you'd normally use directly. However, the Dispatcher, for example, uses this internally. Another example of where this is used is with the awaiter. It's using it when communicating back to the original context. With that being said, the original context or thread is really just accessed using this SynchronizationContext. The instance of the Progress class will capture the SynchronizationContext. This means that whenever it reports the progress, we know that the event is executed on the calling context, which in our case is the UI thread. We can subscribe to the event called ProgressChanged. This event will be raised for each time that we update the progress. To subscribe to this, we will simply introduce an event handler as an anonymous method. The first parameter passed to the event handler is the sender. As we don't care about that, we can use the discard. The second parameter, though, is going to be the stocks that we just loaded and got passed into the progress update. Inside the body of the event handler, we can update and interact with the UI. We'd like to update the value of the progress bar, but also make it a little bit more interesting. I'm going to add some notes in the application to indicate which stock just loaded and how many stock prices that we got with this update. This will now increase the progress bar and then update the notes to tell us which stock it currently completed loading. We did get the entire lists of stocks that we loaded, so we could do even more things with that if we'd like to. There's now a way to handle a progress update. Now we just need a way to trigger this event itself. The best practice when working with the IProgress is to pass the reference into the asynchronous methods. That will indicate a progress change. We'll pass an instance of the Progress<T> into SearchForStocks, which is called from Search_Click. This, of course, means that we need to change the method signature of this method. We'll say that this expects the interface IProgress<T>. In our case, the T is an IEnumerable<StockPrice>, as that's what's going to be passed into the progress update. Each time that we get some stock data back, we want to update the progress of the application. The best way to do that is to chain on a continuation to the Task that's loading all the stocks. We'll also return the data from the continuation to make sure that the rest of the application still works properly. Inside the continuation, before returning the stocks, we want to report the progress. We will make sure that we don't report the progress if we don't have an instance. For this, we can use the null‑conditional operator, which means it will only call Report if the progress is set to an instance. The IProgress interface exposes the Report method. You can use this to report the progress. Anyone subscribing to the event will now get a report that we got an update of our progress. Because we've specified that we have an IProgress<IEnumerable<StockPrice>>, we have to pass a list of the StockPrices. We get this from the Result property that is available on a Task passed into the continuation. As we request some stocks to be loaded, not only does it update the progress in the application, it also adds a nice note to let us know how many stocks were loaded and that we got back to the application. For each stock identifier that we are loading, we update the progress bar, and with very minimum effort, we introduced progress reporting. This gives you an insight into the complexity of introducing progress reporting. There's no built‑in automatic way to report progress, and now you know why. You need to find places in your asynchronous operations where it makes sense to add the progress reporting. In this case, it was rather simple. We could simply report the progress of each stock that was loaded, so depending on the situation it might be easy for you to add this to your particular asynchronous operation, but in other cases, it might be really hard. Given the fact that the tasks themselves don't report on any progress, you'll need to find a way to get this into your asynchronous methods on your own. Even with these limitations, introducing IProgress in your applications gives you a nice and easy way to work with progress reporting.

Using Task Completion Source
Consider consuming a parallel or asynchronous library that doesn't expose the asynchronous operations using the Task Parallel Library, meaning that it doesn't provide you with a task, but it has an alternative way to let you subscribe to when it's done. This could be allowing you to subscribe to an event that indicates when the operation completed. One example of this is the event‑based asynchronous pattern used with, for instance, the BackgroundWorker. In the early days of the BackgroundWorker, the only way for you to know that all the data was loaded was to listen for an event. It could also be used where you're manually scheduling work on the ThreadPool and that thread wants to communicate the result back to whoever is listening. This is where we introduce the TaskCompletionSource. This creates a task which could be returned or awaited. The TaskCompletionSource is then used to indicate that the task has a new result. Let's jump into the exercise files. The Search_Click method, it's awaiting a call to SearchForStocks, and I've made some changes to SearchForStocks. Instead of using the Task Parallel Library together with the async and await keywords, this method now queues work onto the ThreadPool. It's also loading a cached file from disk instead of querying the API. You'll see that there are compilation errors and some other problems that we need to fix. The method needs to return a representation of the asynchronous work. We need a way to grab the result from the thread that loads the file. Only after that is done we want to populate the UI. Loading the file like this is similar to when we used Task.Run, but when we queue the work onto the ThreadPool, we can't await that. We need to introduce the TaskCompletionSource to help us with this. The TaskCompletionSource is generic and allows you to specify what type of result that you're expecting to produce on the task when the operation completes. We are going to set this to an IEnumerable<StockPrice>, as this is what is produced by the queued work. When you create an instance of the TaskCompletionSource, you specify the TaskCreationOptions. It's best practice to use the TaskCreationOption that determines that you're going to run the continuation asynchronously. You can experiment with that on your own and see how different TaskCreationOptions behaves. With the TaskCompletionSource added, how does this fit into the picture? How can we now use this to allow others to consume our work using the async and await keywords. Most important of all, how do we communicate a result from this queued work which will then be set on the task produced by the TaskCompletionSource. The TaskCompletionSource exposes methods. For example, we can set a result. It allows to provide a way to get the task that represents the asynchronous operation. This task isn't running the actual queued work that we've added to the ThreadPool, it's simply a task that is only marked as completed when it gets a result set on it. This means the task produced by the TaskCompletionSource can be awaited somewhere else. If you retrieve the task from the TaskCompletionSource, it will give you the task of an IEnumerable<StockPrice>. Our method can now return the task produced by the TaskCompletionSource. We can call this from Search_Click. We will then have a task of an IEnumerable<StockPrice>. Await this, and you'll get the IEnumerable<StockPrice>, which in the continuation can be added to the UI. We've now removed a few problems and the compilation errors. What's left now is to set the result on the task and for it to be marked as completed. This should be done when the work has finished on the ThreadPool. This is when we tell the TaskCompletionSource that we have a result. We could also set an exception or a cancellation. Now you've communicated to the task that it should be marked as completed and that it has a result available. Whoever is now awaiting this will be able to retrieve the result, and it won't proceed to the continuation unless we have a result from the TaskCompletionSource. It's now loading the file from disk and processing that, then uses the TaskCompletionSource to indicate that it has a result. The file is being loaded using the non‑asynchronous methods, but doing that on a separate thread because we queued the work on the ThreadPool. Let's run the application and make sure that it works. And will you look at that? It worked just as well as it did before. This shows you that you can use the TaskCompletionSource to introduce ways of working with background threads or even event‑based asynchronous patterns, but consume that like the tasks that we've come to really like.

Working with Attached and Detached Tasks
When you introduce parallel or asynchronous operations, you often may have tasks that create other asynchronous operations as well. These nested tasks are also known as child tasks, and the task they originate from are known as the parent. We've got ways to configure how these child tasks operate, and in order to do that, we cannot use Task.Run. I've created a new console application. Let's have a look at how we can configure how the tasks operate. Task.Run doesn't have an overload that allows to configure what's known as the TaskCreationOptions. In fact, Task.Run is simply a shortcut of using the Task.Factory and starting a new task through that. The Task.Factory allows for a whole lot more options to be specified. Let's use the StartNew method on the Task.Factory. Again, this is what Task.Run uses internally. StartNew has a lot more options that can be set. It allows us to pass the Action that will run asynchronously, as well as the CancellationToken. We can then also set the TaskCreationOptions. We could also pass a value that will be sent into the task, which would avoid closures. If we'd like to mimic what Task.Run does, these are the default values it uses. By default, the CancellationToken is set to None unless we pass a CancellationToken. Then it specifies the TaskCreationOptions. For these options, there are multiple different ones. The default for Task.Run is DenyChildAttach. This means that tasks spawned inside our task cannot attach themselves to the parent. We'll have a look at what that means in just a moment. The final parameter is setting the TaskScheduler to the Default scheduler, which is simply going to use the thread pool to queue the work. Using Task.Run is in most situations the right way to go, but this provides a little bit more flexibility. It's only supposed to be used for more complex and advanced scenarios. In most cases, you'll be better off using Task.Run. Just calling Task.Factory.StartNew will simply queue an operation on a thread pool. Now we'd like to configure the TaskCreationOptions to specify how to treat child and parent tasks. Before we do that, let's print something before and after the asynchronous operation. As you see, I can use the async and await keywords in Console applications as well. You've been able to do this since C# 7.1. This is where they introduced the capability of the main method being marked as async. This asynchronous operation that we are starting using Task.Factory.StartNew will start even more tasks inside of it. Then we want to be able to configure how they operate and relate to the parent task. The asynchronous operation will be creating three additional asynchronous operations inside of it, which will all just be simulating some work and then printing a message. Notice that we're not awaiting these operations. By now, you should be familiar with what happens when we execute the application. First, it will say that it's starting, then immediately say that it's completed. After it's completing, 1, 2, and 3 pretty much complete at the same time. Given everything we've learned throughout this course, this makes a lot of sense. It will immediately schedule the work for each nested task that the parent task creates. The parent task is marked as Completed as soon as it's started all of its nested tasks. Let's change the behavior of how the parent task creates the child tasks. We can configure that we want to attach each child to the parent task. This very long documentation is stating that by default, the child task, also known as the inner task, created by the parent, also known as the outer task, executes independently of its parent. You can use the option AttachedToParent so that the parent and child tasks are synchronized, although the documentation is also stating that if the parent task is configured with DenyChildAttach, setting this option has no effect. DenyChildAttach is a default value for Task.Run, which means that nested tasks cannot attach themselves to the parent when using Task.Run. This means the child tasks would then work as detached tasks. If we just attach the first one and run the application, the output is a little bit different. First, it says Starting. Before printing completed, it will say that the attached task completed, which is a different behavior from before. Finally, the rest of the tasks complete. It didn't run the continuation until the attached child was completed because the parent and one of the child tasks are now synchronized. Let's now attach all of the nested tasks in the operation. With that being changed, the continuation that states that the operation completed should now only execute once all of the nested tasks complete, and this is exactly what the application is doing. We didn't reach the continuation for the outer task until all of the attached child tasks were all done. This provides a lot of flexibility. If we were now to change the parent task to use DenyChildAttach, we're back at the same behavior we had in the start, which means none of the child tasks were able to attach to the parent. Let's remove all of the TaskCreationOptions and all the nested child tasks to explore another situation where creating a task with a Task.Factory.StartNew is different from using Task.Run. We can introduce a new child task that simulates a little bit of load and then returns a value. If we'd like to await this child task inside the anonymous method before returning a value, we'd use the async and await keywords. Of course, this will generate a bit of code behind the scenes. That means that it might have a slight performance implication, but it's worth it because we can now await this call. We start an asynchronous operation using the Task.Factory. It has an asynchronous anonymous method which awaits an asynchronous call. What do you think StartNew will return? The return type is in fact Task<Task<T>>. If you await the task produced by StartNew, this will give you the child task. You could then add another await to retrieve the value of that inner task. This is different from what you'd get if we change Task.Factory.StartNew to Task.Run. If you do, the return type is instead Task<T>, and we can remove one of the await keywords. We saw earlier that calling Task.Run constructs the task using the Task.Factory with a few default values. What it will also do is automatically unwrap. That means when we want to get the result of the operation, normally you just await the task produced by Task.Run. Let's change the code back to using Task.Factory.StartNew. What's returned from the anonymous method is a Task<string> because it's marked as async. Since we wrapped this asynchronous operation, it totally makes sense that we'll end up with a Task<Task<string>>. So how come Task.Run provides a different type? It's because it's calling the method Unwrap, which produces a proxy to give you a Task<T> instead of a Task<Task<T>>. When using Task.Run, you don't have to worry about this because it automatically unwraps. As we've discovered, the Task.Factory allows for more configuration options, and in those situations it's important to know how the Task.Run and the Task.Factory.StartNew are totally different. When you unwrap the task, you no longer need the two awaits. If you need to specify the TaskCreationOptions and use the Task.Factory, you'll run into all of these situations that Task.Run solves for you. Another important feature of Task.Factory.StartNew is that it provides a way to pass an object to the asynchronous methods without introducing a closure, which then avoids unnecessary allocations. Let's say that you create an instance of an object somewhere outside your asynchronous operation. You then want to use this inside your method body of that operation. If you simply point to that variable, that will introduce a closure, which generates code and possibly unnecessary allocations. To avoid this, there's an overload that we can use with the Task.Factory. We can, in fact, pass an object to be used inside the action delegate. You'd have to specify now that the action expects a parameter passed to it. This would be of the type object, which means you'd have to cast it to the appropriate type before using it. This avoids unnecessary allocations by avoiding a closure. We've now explored how flexible the Task.Factory is and that it's great for complex and advanced scenarios. You will in most situations be better off using Task.Run.

Summary
In this module, we covered so many interesting and useful topics that will hopefully make you a much better developer when it comes to introducing asynchronous programming with the Task Parallel Library. We started off talking about the complexity of introducing proper progress reporting and saw how we can use I Progress<T> to make the process a little bit easier. There's no built‑in support for reporting the progress of a task, so you'd have to handle this on your own. You then learned about the TaskCompletionSource, which is tremendously useful. If you're using the thread pool to manually queue work or use older event‑based asynchronous patterns, the TaskCompletionSource will provide a way to represent this work with a task while also allowing the application to make use of the async and await keywords with this operation, even if it didn't originally use the task to run the operation. You now also know that Task.Run comes with some predefined behaviors, and it's a little bit different from Task.Factory.StartNew. If you want total control and flexibility, you can tweak how the child and parent tasks operate. Instead, use the Task.Factory.StartNew and configure that. You can attach the child tasks to the parent tasks and only have the parent task marked as completed as soon as all the nested tasks have all completed. Many of us have used the TaskFactory in older versions of .NET. Nowadays, in most situations, using Task.Run will be completely sufficient. It ultimately depends on what you want to do. We've covered so much in this course around how to properly apply asynchronous programming in your C# and .NET applications. By now, I really hope that you have a good understanding of how to work with async and await, the Task Parallel Library, and ultimately, how to make your applications a lot better. All that we want is, of course, to give our end users a really good experience. With all this new knowledge, you are ready to tackle different situations where applying asynchronous programming is needed. It's now time to switch gears and talk about parallel programming and building multithreaded applications.

Parallel Programming and Multithreading in C#
Multithreaded Programming in C#
Welcome back to the course Asynchronous Programming in C#. My name is Filip Ekberg. Do you ever find that you have large collections of data where each item could be processed independently to speed things up, but you just don't know where to start? The next few modules will focus on teaching you how to break it down and introduce parallel programming. This will set you off to build really powerful and fast applications. While the task certainly is a way to introduce a parallel operation, it's not automatically going to make use of all the available resources such as the CPU cores or available threads. An asynchronous operation is operating in parallel, but the concept of parallel programming is slightly different. The biggest difference between parallel and asynchronous programming is that in asynchronous programming, we schedule a continuation. Later in this course, you will learn how to use these two principles together. Essentially, parallel programming allows you to break down a problem. It may be a large problem or a small one, but where each item takes a little bit of time to process. Doing so sequentially would take a much longer time than needed. You've learned how to use Task.Run and know that it relieves the current thread of work. This would certainly give the user the impression that it solves a problem faster. The goal though is to compute each part independently. This means we won't introduce one task, but each piece that can be solved at the same time would do so in a separate operation. Ideally, we'd get some help from the framework to determine things like the most appropriate degree of parallelism. Parallel programming in .NET can be achieved in a few different ways. There's the lower‑level thread class, which gives you complete control of an actual thread. With complete control also comes great responsibility. You'd have to write a lot of code on your own to effectively introduce parallelism using the Thread class. Then we have the Task Parallel Library, which provides an abstract concept of looking at an operation that shouldn't block. The task should be the preferred way of running operations where you don't want to block the current context. With a task, you don't have to care about if it's a new thread or if it's reusing an existing thread. The task can schedule work on a thread pool, which may or may not spawn an actual thread. It may also reuse a preexisting thread. This means that you don't have to use the lower‑level threading and can instead rely on the framework and the language to give you some help. The Task Parallel Library also introduces helper classes to easier introduce parallelism in your code. You've got a class called Parallel, which provides functionality for running a set of operations across the available resources, but also ways to easily convert a For and a ForEach loop into a parallel loop. When processing collections, you may have encountered LINQ, and there's a very handy extension with the Task Parallel Library, which provides a way to write parallel LINQ. All the parallel helpers in the Task Parallel Library are built on top of the task. When Microsoft introduced and started building these features, they were commonly known as the parallel extensions. Obviously, there are also other libraries out there that will allow you to run background work and process data asynchronously. It's time to explore the parallel portions of the Task Parallel Library.

A Problem to Solve in Parallel
Let's say that we have a rather large list of stocks, and you need to perform some calculation on each of them. They have nothing to do with each other, and the calculation can be done independently. To simulate a little bit of load, I'm going to introduce a method that simply spins a loop for a few seconds before it's returning a value back to the caller. If you call this method on a thread, that thread will be busy running the loop for the given time. If we call this method on multiple different threads, which is the goal, that would consume 100% of the CPU available on the system until the operation completes. This will ensure that the problem is solved as quickly as possible. Our computers are most definitely different and have different CPUs. We're going to write code that effectively distributes work across all the available resources. No matter if I run the code or if you run the code, we can be sure that it will run as effectively as possible on each system. If we call this method, it will produce a result as a stock calculation. This just contains some fake information for us to populate the application with. It will also contain the amount of time it took to perform the operation. This application provides some fake stock data, which we're going to process. I've already added an in‑memory collection of data to make it a little bit more interesting. The data will generate a random amount of stock prices and some data inside of it. Exactly the contents of this isn't really the important part. What we are going to focus on is speeding up the application. These are all completely independent from each other. Therefore, we could assume that the processing can be done completely separately. Each stock identifier presents a chunk of stock prices. In our example, it's just some generated prices. We can process these chunks in parallel, which will speed up the process of analyzing them roughly if I have 12 available cores, I could process 12 of these at the same time. That's including processing one on the UI thread. These operations are CPU‑bound, and that's the perfect fit for parallel programming. If, however, most of the time were spent waiting for the operating system to return file contents or a result from a web invocation, that would be an asynchronous process, and then using the asynchronous principles would be more appropriate. Before we introduce the parallel principles, let's first add the code in a non‑parallel matter to see how it will later improve the experience of the application. I'm going to compute the result for each of the different stock identifiers to simulate a little bit of load. Once all the different stocks have been processed, we'll add the result to the UI. Running this and invoking the Search button will run our piece of test code. Don't worry about searching for an actual stock. That code hasn't been added. You'll notice that it takes a fair bit of time to process each of the stocks. It will also lock the UI until all of them have completed processing. This is, of course, because we're doing all of the work on the UI or the main thread of the application. Your first urge to solve this is maybe to introduce a Task.Run and wrap all the heavy code in a new task. However, the problem with that approach is that you'll only move the work to one additional thread. You will then have one thread that takes care of the UI and one thread that processes the work. The user experience will be improved, but the process of the operation isn't really faster. Since all of the calls to the method calculate are completely independent, wouldn't it be great if they were all processed in parallel? And certainly that's the goal. You could run each of them in a separate Task.Run, but you'd end up writing a lot more code than you have to in order to, for instance, handle each continuation. Instead, we're going to use the Parallel class. This introduces a few helpers that will allow us to more easily approach this. Internally, it will, in fact, use the task from the Task Parallel Library. It will also provide some configuration capabilities to override some behavior. Out of the box, it will take care of calculating the most efficient way of dividing the tasks it's given among the available cores. This means distributing the work effectively across the different cores that you have available on your system. If we simply spawn a lot of tasks using Task.Run, not only will it not be as efficient as it could be, it will also require us to write a lot more code than we have to. Instead of doing that, we can use the methods available on the Parallel class. We could use Parallel.For, which is really the same as a normal for loop, but it will run in parallel and be configured to distribute the work according to the system it's running on. You could also use the Parallel.ForEach, which is a parallel version of the normal foreach loop. These two will be covered later. You've also got a method called Invoke on the Parallel class. This takes a list of actions. These actions will be distributed and executed in parallel. None of the methods on the Parallel class will guarantee that it's executing in parallel because it really depends on the system. I've had a look at the internals of Parallel.Invoke, and depending on how many actions you pass to it, it might execute them using a Parallel.For loop This just proves that it does some really heavy lifting and provides some great functionality and optimizations, which you shouldn't take lightly. Parallel.Invoke doesn't return anything, and the actions you pass to it are distributed to run as effectively as possible and run independent from each other.

Your First Parallel Operation
To make our code run more effectively and introduce parallel principles, there are a few things that we need to do. We can see that we require the return value from each call to Calculate, which we then used to populate the UI. Because each action will run Calculate on a separate thread, we need a thread‑safe collection which allows for data to be added once it's been processed. We can introduce a ConcurrentBag of StockCalculation. Each parallel action can then add values to this. We'll also make sure that the data grid in the UI is now using this list as its item source. What's left now is to use Parallel.Invoke to run each of the calls in parallel. To make it simple for you to understand the process, we can introduce these actions inline and pass them to Invoke. Create an anonymous method. Each of the anonymous methods will call Calculate and add its computed value to the ConcurrentBag. This now means that each operation will hopefully run in parallel. If you've got enough cores and threads available, they should all complete at the same time. As you run the application and press Search, the operation now completes a lot faster. They were, in fact, processed in parallel, at least on my system they are. And this may, of course, differ depending on the system you're using. You might have observed that the UI locks up. This is because the call to Invoke, the Parallel.For, and the Parallel.ForEach are all blocking the calling thread. Let's later explore how we can solve this. If you already have an idea on how to solve this, feel free to pause the video and try that out. There's no guarantee that the actions start in the order you've passed them in. The whole idea is that we don't care. We're now introducing all of these calls to process these chunks of data in parallel, and we just want that computed as fast as possible. No matter if you're using Parallel.Invoke, Parallel.For, or Parallel.ForEach, they will all help you to distribute the workload in the smartest way possible across the different cores on your computer. This is going to be a much more effective way of approaching the problem rather than introducing a bunch of Task.Run to make all of this yourself. As a word of caution, since the call to Parallel.Invoke, For, and ForEach are all blocking the calling thread, that means if the actions were to use something like the Dispatcher and forcing itself back to the calling context, that may result in a deadlock. So really avoid doing that. These different methods on the Parallel class also provide some configuration options. The first parameter that we can pass to Parallel.Invoke can be an instance of ParallelOptions. The ParallelOptions allow us to pass a cancelation token, which means the parallel execution can be canceled. Remember, they're all using tasks internally. This means we'd have to handle the cancelation just as we'd handle the cancelation in an asynchronous operation. We also have something called degree of parallelism. This will allow you to change the maximum number of concurrent tasks. What do you think happens if I change this to 2? I've got 12 cores on my machine, so it can handle a lot of work. However, if I don't lower the number of concurrent tasks, it may consume all my available resources on the system. I'm not suggesting or recommending that you manually change this to a lower setting as it's often the intent to solve a problem as fast as possible. In the case where you know that the operation is CPU‑intensive for a really long period of time, it may, however, be a good idea to lower this to avoid a very unhappy user. If you run this with a lower setting, you can see that it takes a little bit longer to complete the operation, at least comparing to when it used all the available resources. Consider the case of there being hundreds of stock identifiers to be processed. Lowering this will have a drastic change to the performance. You can set a breakpoint in each of the anonymous methods that are passed to Parallel.Invoke. Once the application runs, you can see that the max degree of parallelism is set to 2. It will therefore start two of them. As they complete, another two will start. Of course, our operations happen to take roughly the same amount of time to process. If that wasn't the case, it would start the new operation as soon as one of them have finished. This shows some of the really great benefits of using the Parallel class available in the Task Parallel Library. As opposed to building something on your own, using tasks, or even the lower level concept of a thread, this provides an out‑of‑the‑box solution, which helps you to distribute parallel work. As we've seen, Parallel.Invoke is really powerful. It enables you to execute parallel operations. We can configure exactly how many tasks that we want to concurrently run. But in the most cases, you want to keep that to the default setting. If we don't set this, the methods on the Parallel class will run as effectively as possible, depending on the system that executes it. A word of advice. When using this in ASP.NET, if you misuse the parallel principles, that can cause some really bad performance for all of your users. Just imagine if you have an invocation from a user where it runs something that utilizes all the available cores on the server. What do you think happens when all the other users want to consume the same system? That's something that you should keep in mind. If you really want to do heavy computation on the server based on an invocation, there are multiple different architectural decisions that you can make in your application, but that's totally out of scope of this course. We easily introduced a parallel invocation in the application to take some operations that were rather long‑running and CPU‑intensive and completed that in parallel. It's now executing all of this as effectively as possible based on the system that runs the application. We've still got a lot to learn to fully understand how to effectively apply these parallel principles in our C# applications. As you proceed, you will learn about combining asynchronous and parallel principles, as well as using Parallel.For and the Parallel.ForEach loops, and really reduce the amount of code that you have to write. At the same time, provide additional functionality, which will help you write better and faster and as well easier‑to‑maintain code.

Using Parallel and Asynchronous Principles Together
The difference between parallel and asynchronous programming may seem subtle. In essence, asynchronous programming allows for a way to run a continuation as the operation completes. The asynchronous operation runs in parallel and doesn't block the current thread that starts it. The purpose is often to perform an I/O operation, meaning reading from disk or calling a web API. With the task in the Task Parallel Library, we've got the capability of doing both asynchronous and parallel programming. I like to think of the task as a way to represent work that needs to execute somewhere other than the calling context. We know by now that when we use Parallel.Invoke, Parallel.For, or Parallel.ForEach, we can distribute work across different cores and threads available on that system. Calling these methods block the current thread until the entire operation has completed. Wouldn't it be great if we could move that entire operation into a different context and then somehow subscribe to when all of that has completed? Sounds familiar? It definitely should. If we simply wrap the entire Parallel.Invoke call in a new anonymous method, which is passed to Task.Run, this would mean that the calling thread won't be blocked. This also means that we can use the async and await keywords to say that we'd like to introduce a continuation, which is only executed once all the parallel operations have completed. You can run this, and the application certainly behaves as it did in the previous clip, although the user experience is greatly improved, especially because the UI doesn't freeze. However, this also introduces a bit of a problem. When we move the call to Parallel.Invoke to another thread, that means we wouldn't have that thread available to work on, which ultimately means that it's not as efficient as it could be. It really comes down to your intention, and sometimes you have to make a trade‑off to allow a certain user experience. Users may be fine with an operation taking a little bit longer given that the UI doesn't freeze. If a UI freezes or locks up, I know from my own experience that I'm pretty quick to kill the process. The change you have to make to the code to combine asynchronous and parallel principles are really small, and it's pretty easy to combine these paradigms. This is hugely thanks to the Task Parallel Library. If you had to build this on your own using the lower‑level threading or even using the thread pool to queue some work, you'd end up with some really complex and error‑prone solutions. Not only do we get help from the Task Parallel Library, let's not forget about the async and await keywords that also uses this internally to help you build better code. We've improved the processing of the stocks. We have offloaded the UI thread from the work. But what happens if the parallel operations fail? We need ways to handle potential exceptions and problems. Next up, we're going to learn about how to handle these exceptions.

Handling Exceptions
What happens if one of the parallel operations throw an exception? With a task, they are normally swallowed, and you must await it or introduce a continuation to properly handle the exception. With the parallel methods, it's slightly different. While it's true that the parallel methods used the tasks internally, you don't have to do anything in particular to receive the exception in the calling context. The method call to Parallel.Invoke, Parallel.For, and Parallel.ForEach are blocking calls that also validate the parallel operations for you. If there's an exception in any of the parallel operations, they will be caught and rethrown as an aggregate exception. To show you how this works, we can wrap the Parallel.Invoke call in a try and catch block. Then to raise an exception, let's simply throw a random exception in one of the parallel operations. If you set a breakpoint in the catch block, then run the application, as you search for a stock, it should now trigger this breakpoint. This will give you an aggregate exception that contains the exception from the parallel operation as an inner exception. How about if every parallel operation that is executing throws an exception? What do you reckon will happen then? You would then get all of the thrown exceptions in the catch block. This is great and will allow you to properly handle if there's one or many operations that fail. All right, we now know that we can catch the exceptions and that they were thrown on the calling context as we'd expect. However, I bet you're now wondering what will happen to not yet started parallel operations. You can experiment with this by setting the degree of parallelism to either two or even one, then set a few breakpoints to see how the execution flows. Let's also remove a few of the thrown exceptions just to make the methods a little bit different. There are now two parallel operations that both throw an exception. But as you continue the execution, what might surprise you is that the next operation is started even if the previous one failed. Therefore, throwing an exception doesn't cancel any parallel operations. That are queued. What is also interesting is that the exception isn't thrown until the entire collection of parallel operations have completed. It only validates all of the operations once they have all completed. That means we won't reach the catch block until all of these operations have completed. Depending on what you expected, this behavior might be good or bad. If you expected the following operations to be canceled, that's something that we were going to cover in the next module. But throwing an exception did not cancel any queued operations. In this case, our parallel operation was started from an anonymous method executed by Task.Run. It would be appropriate to rethrow the exception or throw a new one inside the catch block. This will ensure that whoever is awaiting this parallel operation or actually the task that represents it will know that there was a problem. I don't particularly like nor do I recommend that you blindly swallow exceptions without letting the consumer know that there was a problem. With this understanding of Parallel.Invoke, how exceptions are handled, and how to combine this with asynchronous principles, it's now time to look at the Parallel.For and Parallel.ForEach loops.

Processing a Collection of Data in Parallel
You've seen how to use Parallel.Invoke to run a set of actions that will be distributed across the different cores and threads available on your system. This is to effectively deal with chunks of data to process them in smaller pieces. Instead of using Parallel.Invoke, we are now going to explore two different approaches. The code that we've written so far isn't really flexible enough. If we add more stock identifiers that we'd like to process, doing so would require us to change the code. Obviously, this isn't dynamic enough, and the code we've written so far isn't really production‑ready. How about we use a loop to iterate the stocks instead of manually creating these actions with anonymous methods? Refactoring this code to use a for each or a for loop will be pretty easy. We've got a lot of similar code snippets here, which should indicate that we can make this a lot prettier. The Parallel class exposes a method that allows us to define a foreach loop. This method has a bunch of overloads that will allow us to introduce custom behavior. What these all share is that they expect a source of elements to iterate, as well as the body that will execute for each element. This sounds and, in fact, is very similar to how a normal foreach loop behaves. The main difference is that it will automatically scale the work across all the available resources on your system. This means if we have hundreds of elements that we could process in parallel, the runtime will figure out the most suitable way to distribute this work. Hence, it will compute the degree of parallelism. Then it will run the body for each element in parallel. We don't have to worry about threads or the task. This method call is solving all of that under the hood. This is what makes it a very powerful library. The first parameter that is passed to the foreach loop is the source. This could be any IEnumerable. We can pass the dictionary that represents the search result of stocks. The second parameter is the body for the foreach loop. This is represented as an action, and we could point this to a method as well as long as it matches the signature, but we're going to use an anonymous method. The method or action will receive a parameter. Can you guess what this parameter will contain? It'll, of course, contain the value we're currently going to process in that iteration, much like the element in the body of a foreach loop. The body of the foreach loop or rather the body of the anonymous method will now just contain somewhat the same code we had in each anonymous action that we pass to Parallel.Invoke. We'll perform the calculation for that stock. Then the result of that calculation is added to the thread‑safe collection, which we can later use to update the UI. We can now get rid of the previous code that uses Parallel.Invoke. This is, in my opinion, a much cleaner and easier to read solution. As you run this, the application behaves just as it did when we used Parallel.Invoke, which is providing a really nice user experience. The computation was done as fast as possible using all the available resources. If we now extend the list of stocks to process, it would just work without having to do anything extra. We've also continued to use both the asynchronous, as well as parallel principles together as we have now wrapped the Parallel.ForEach call in a Task.Run. A funny side note is that on my computer, this is not even utilizing all my available cores. As this list grows larger with more items, the difference between the parallel and non‑parallel versions will therefore be a lot different. We rather easily converted this to use a Parallel.ForEach loop. Just as with Parallel.Invoke, you can pass parallel options, which would contain a configuration for the degree of parallelism or the cancelation token. You can experiment with the degree of parallelism and change the amount of concurrent tasks that we're allowed to use with Parallel.ForEach. This makes it very flexible. And with very minimum effort, we did make the application a lot better. Parallel.ForEach, as well as Parallel.For, both return something called a ParallelLoopResult. This return value will tell you if the execution completed successfully. Completing successfully means that the loop ran to completion and that it didn't receive a request to end prematurely. Ending prematurely means that an integration requested the loop to break or stop. The ParallelLoopResult also contains a property for the lowest break iteration. This is just relevant if you're using Parallel.For as it represents the lowest index of where a break was called. You could have multiple parallel operations that have already started. When one of them requests to break, it won't automatically stop the others from running. Let's look at how we can break out of a loop. You can't simply use the break keyword like you do for a normal loop. You must take a slightly different approach. You also need to make sure that consecutive operations won't start executing once you requested a break. This is just another way of saying that we don't want to start the rest of the parallel operations that are waiting to be processed. The action that represents the loop body accepts a second parameter of type ParallelLoopState. This parameter is a representation of the loop state, which will provide a way to communicate that we'd like to, for instance, break or stop the loop. You can call the method state.Break, and that means it will only cease the executions of iterations beyond the lowest iteration index. What is also important is that it's breaking at the system's earliest convenience. Let's assume that you have 100 elements to process in parallel. If you request to break at the index 50, that means all the iterations with that lower index than 50 should still run even if they have not yet started. Anything above the index 50 will not be started. This is why the lowest break iteration is particularly interesting. There's another method available on the ParallelLoopState that would allow you to cease the execution of further iterations. This method is called Stop. If you called Stop, it will communicate that it should cease the execution of the parallel loop at the system's earliest convenience. If you call Stop, other already executing iterations will be able to check the state property to see if IsStopped is set to true and could then handle that and exit gracefully. Let's break when we find one of the stocks that we know are available. We also have to make sure that the code beneath this won't execute. Calling state.Break is just a normal method call, so either we wrap the code below in an else block or we'll just return. It won't automatically exit the current context. Before you run the application, make sure that you set a rather low degree of parallelism and add a break point. As you search for the stocks, it will hit the breakpoint, and you can see that the loop result has the IsCompleted property set to false. This is because the loop didn't, in fact, complete. It detected a break. As we are still using the Parallel.ForEach loop, the LowestBreakIteration property is of no value to us. What may come as a surprise is that it proceeds to run the next parallel operation as well and then finally stops. This is because it's breaking at the earliest convenience. You can change the degree of parallelism to 1 and then break on the first stock. Doing so gives you a little bit of a different result. Can we make this better? Could we detect that a break has been requested and gracefully handle that? Definitely there's a way to check if another iteration has requested to break. The state parameter passed to the loop body has a property called ShouldExitCurrentIteration. This will be set to false unless another iteration has requested to break. If the previous iteration requested a break, it will be set to true. Now, the result is a little bit different and probably more so in line with what you might have expected. This will cease the execution of the iterations that have not yet been executed. We've been using the Parallel.ForEach, but using a Parallel.For loop is equally simple. It also provides the same functionality and configuration capabilities. If you know how to use the Parallel.ForEach loop, you'll definitely feel right at home with a Parallel.For loop. Just like a normal for loop, it provides a way to iterate from one index to another. The from index is an inclusive number, while the to one is exclusive. That means if we'd like to iterate from 0 to and including the index 9, we'd construct a Parallel.For loop that states 0 and 10. Obviously, there's a loop body just like the Parallel.ForEach loop. This provides the index instead of the item as we saw with the foreach loop, as well as the state. I'd suggest that you experiment with this loop, and we'll use it more further down in this course. At a first glance though, it's very similar to how you'd use the Parallel.ForEach like we just experimented with. Not only is this very powerful, it's also very flexible, and it's built on top of the Task Parallel Library, which we've seen is extremely powerful, and it allows us to do simultaneous processing. All of this illustrates how easy it is for us to introduce a parallel iteration in an application to process lots of data. We've still got a lot of topics to cover, such as blocking, deadlocking, shared variables, canceling parallel operations, and parallel LINQ.

Summary
This module has been packed with information about parallel programming. Hopefully, you've got a good understanding of when it's appropriate for you to use parallel programming in your applications. This means understanding the implication of introducing things like Parallel.For, Parallel.ForEach, or Parallel.Invoke. If you're working in ASP.NET, it's really important to make the conscious decision that this is something that you want to use in your code. By now, you know the difference and similarities between parallel and asynchronous programming. The big difference is that with asynchronous programming, you schedule a continuation to handle the result of the concurrent operation. The methods we've explored in this course all depend and build on top of the task in the Task Parallel Library. You can use the Task Parallel Library to introduce both asynchronous and parallel principles in your applications, which makes this an extremely powerful toolkit when you write your code. Remember that all of these different principles work in any type of C# and .NET application. Although if you're working on a microprocessor that's using .NET like a coffee machine or something that only has one core, maybe using asynchronous and parallel programming isn't completely necessary. We introduced parallelism in an application with the help of the Task Parallel Library, which is an extremely efficient way for you to break down a large problem into smaller pieces that are all solved independently. This saves you from managing the tasks, threads, and computing how many cores there are available for you to construct the ultimate parallel execution. The Task Parallel Library and its Parallel class does this for you. It distributes the workload in the most efficient way possible. You could now even configure the execution to lower the maximum amount of concurrent tasks. That will be handy in the cases of you not wanting the application eating up all the available resources. We introduced the ConcurrentBag<T> to allow for an aggregated result. This provided a thread‑safe collection that each parallel operation could add data to. Most important of all, you learned how to take a big chunk of data, split that into smaller pieces, and run computations concurrently. Because the Parallel class and its methods block the current thread, we saw how to use parallel and asynchronous principles together. You simply wrap the operation in a Task.Run, although, when doing so, you're also not using the full capabilities of the available threads. This is the right call, especially in a UI application. If this was a console application, that would be a completely different story. Next, we are going to dig into the advanced topics and cover shared variables, locking, deadlocking, and canceling parallel operations, as well as parallel LINQ. There's much more to cover, and you're off to a really good start.

Advanced Parallel Programming: Understanding Locking and Shared Variables
Advanced Concepts
Welcome back to the course Asynchronous Programming in C#. I'm Filip Ekberg, and now we're going to look at some advanced topics in relation to parallel programming. This module will teach you about sharing variables between parallel operations. This may sound trivial. But to achieve this, you're going to have to learn about locks, which ultimately requires that you understand why and how a deadlock occurs. You will also see how you can perform atomic operations in a thread‑safe manner, which will greatly improve the readability of the code compared to introducing a lock object. Then, you will see how a cancelation is a little bit different when it comes to a parallel operation in comparison to what you've previously seen with a task from the Task Parallel Library. Finally, this module will teach you about thread local and async local variables. These are extremely handy. But if not understood correctly, they can introduce some interesting bugs and be rather confusing. Continue through this module to learn about these important topics.

Working with Shared Variables
You looked at thread‑safe collections that you can use to aggregate data in the application. This allows different asynchronous or parallel operations to add its result to the collection without worrying about raised conditions or losing data. The thread‑safe collections are great for lists of data. But how would you go by sharing a variable that are used and changed across different threads? This is for situations where you perform calculations in parallel. Let's have a look at what happens when you try to use a normal variable that will be shared among the different threads. You're going to update its value from different parallel operations. We're going to leave the Windows application for now and explore this in a new console application. To be able to compare the parallel and non‑parallel versions, let's add a stopwatch to calculate the number of elapsed milliseconds. Then at the end of the method, we'll print the elapsed time. To understand the problem we may encounter when working with shared variables, we are first going to introduce a normal for loop that, in each iteration, calls a method to compute a given value. This for loop will run from 0 to 100, meaning the last element to process will be 99. Add a new method called Compute, which just simulates that It takes a bit of time to process the value. We can add a bit of code that runs a while loop for a number of milliseconds before returning a value to the caller. Of course, this portion of the code is something that I'd recommend you to remove in production code. This code will just make sure that it takes some time to run the method, and each call to it will take a different amount of milliseconds to compute. If we now simply add the result of the invocation of this method to a variable that we call Total, we can then print this at the end of the main method. Running this will give us a result, which did take a bit of time to process. The result of this operation should be 5000 no matter how many times you run the application. This operation took approximately 3 seconds to execute. We can now change this to use a parallel operation instead. Each call to compute takes a different amount of time, and each call is completely independent from each other. However, each iteration also acts as the same variable as all the other iterations. This means we have a shared variable that we interact with after each call to Compute. I don't want to refactor the application too much. And at this point, I'd just like to change the for loop to use a parallel loop instead. Using the for loop and the parallel version of the for loop are very similar. The first parameter is where we want to start off. We'll start from 0 and go to 100. Notice that it says toExclusive. Just as with the for loop, this means that the last value will be 99. The final parameter is the action, which can be an anonymous method or a method call. The body of the anonymous method will be exactly the same as our non‑parallel for loop. It will call the method Compute and then add the computed value to the variable total. If we now run the application, it pretty much completes in an instant. After about 140 milliseconds, which is almost 20 to 25 times faster, we have a result. But hold on. Can you spot the problem? The result is no longer 5000. This means we've got a bug. The application definitely computes the value much faster, but it's not giving us the correct result. It doesn't matter if we can compute the result super fast if it's incorrect. The problem here is that we are interacting with a variable total from a lot of different threads at the same time. With this code, we can't guarantee that only one thread interacts with the value at a given time. This also means that if we rerun the application, we may get a different result. We need a way to fix this and to be sure that only one thread at a time changes the value of the shared variable. To achieve this, we must use something that allows us to increase the value in a thread‑safe manner. Because the shared variable is a decimal, this means we cannot use any of the atomic operations. How to use the atomic operations with the interlock class is covered later in this module. We're going to instead introduce a locking mechanism. Think of this process as locking a door to a room. Inside the room is the interaction with a shared variable. Only one thread can enter this space at a given time. In C#, we can use the lock key word together with an object, which is often referred to as a sync root. The sync root or lock object is used to make sure that only one thread can execute the code inside of the lock statement. Every other thread that then tries to enter this locked space will be blocked until the lock is opened. The best practice is to introduce a locked object and only lock it for as short of a time as possible. I normally introduce my lock object as a static variable in my class, and I call it syncRoot. We can now use the lock, which enables us to safely update the shared variable. What's happening is that we're now locking the door. We are saying that we want to lock syncRoot. So we can be the only ones that currently work with this object, meaning that whenever someone else tries to lock this, they will have to wait for it to unlock this door. That means that all the code inside this code block can safely update and work with a shared variable. If you run the application now, you can see that it takes about 3 seconds again. Do you know why this is performing as badly as the normal sequential operation did with the previous for loop that we used? This is because we're not following the guidelines of doing as little work as possible inside the locked space. This code is, in fact, saying that we can only run one call to compute at once because it's being done within the lock. If we instead introduce a temporary variable in the parallel operation and only lock the door when we interact with the shared variable, the result should be very different. It performs the computation outside of the locked door. And then after it's computed and we have a value, it's updated within the locked door in a thread‑safe manner. This allows you to update the shared variable without locking the syncRoute or lock object for too long. Now it takes about 150 ms, and we're constantly going to get the result of 5000. This means we greatly improved the performance of the computation. And at the same time, we now know how to safely update and work with a shared variable. As a word of advice, be very careful with the locks as it may lead to deadlocks, especially when you're using nested locks. If you're not paying attention, your application may end up deadlocking. We'll investigate that later on in this module. I'd also like you to consider how to approach locking in the most efficient way possible. Ensure that you do as little work as possible in the locked portion of the code. This will allow other threads to access the shared variables. Try to think about how you can optimize your code to have as little side effects as possible, especially when we introduce parallel and asynchronous patterns. To summarize, we saw that it's rather easy for us to interact with a shared variable in our parallel operations. We also saw that there might be some side effects if we are not thinking about what we are doing. Next, we'll look at atomic operations in a multithreaded application.

Performing Atomic Operations
If you want to perform atomic operations in a multithreaded application on, for example, an integer to increment, decrement, or add, you don't need to introduce a lock like we've just seen. There's a handy class in System.Threading that allows you to perform atomic operations for shared variables. This is suitable for multithreaded applications. The class Interlocked exposes a lot of functionality to perform atomic operations on 32‑bit and 64‑bit integers. Let's say that you want to increment a variable with 1 in each iteration of a parallel loop. Doing this with our previous approach would require a lock object, a lock and the code to interact with a variable. We can simplify this and use the Interlocked class. Using Interlocked.Increment and passing a reference to the integer that we'd like to increment with 1 will ensure that only one thread interacts with this at the same time. You could use this, for instance, to track failure or successful attempts to some interactions in your parallel operations without even having to introduce a locked object for the shared resource. This means we have less code. You could also use Decrement, and can you guess what the result would be in this given case here? I'm sure that you guessed that this will return ‑100. If we'd like to do something similar to what we did earlier where we add to a total variable when we get a result back from Compute, we can also use the Interlocked class. Because the Interlocked class only exposes atomic operations, you won't be able to use it with a decimal. It only works with integers. This means we are going to lose a little bit of data because we have to cast the result that's coming back from compute to an integer. But for the purpose of this demonstration and this exercise, I'm okay with that. Interlocked.Add takes a reference to the variable that you want to add a value to. After it retrieves that value, it will add the second parameter to that value and simply replace the value in the referenced integer. Really, this here is just a thread‑safe version of doing total += result. If you do this, you can see that the result is now 4950 and not 5000 like we've seen earlier. This is because the decimal returned from Compute had to be cast to an integer, and thus we lost some data. Although each time we run the application, it yields the same result. This means we introduced a thread‑safe approach of adding two integers. In my opinion, this is a much cleaner approach, although it does only work with integers so that's really important to keep in mind. You could also use the Interlocked class for niche and specific situations. There's an Interlocked.And, as well as Interlocked.Or, which allows for you to do bitwise operations. You also have Interlocked.Exchange<T>, which is the only atomic operation that allows you to work with a reference type. This generic method allows you to exchange the value in a shared reference type and guarantee that no one else is exchanging at the same time. It will also return the original value. There's a non‑generic version of the Exchange method as well. This you can use for integers and doubles. I suggest that you experiment with the Interlocked.Exchange method. An example use case that you can try and implement is to use it as a gatekeeper instead of using a normal lock object. Think of having an integer that represents open or closed. With the introduction of the Interlocked class, you now have a cleaner approach to update and work with shared variables with the given constraints. There are benefits of using the Interlocked class and the atomic operations it provides, although a locked object provides much more flexibility. Next, let's discuss how to introduce a deadlock, which is clearly something we'd like to avoid. Understanding how it occurs will definitely make you a better developer.

Deadlocks with Nested Locks
Previously in this course, we saw that a deadlock can occur rather easily when using async and await. To achieve a deadlock with the lock keyword, it's a little bit more complex. However, it can occur, and it's a good idea to understand why. The lock keyword will ensure that if the lock object is already locked, the thread is locked until that lock object is unlocked. The reason a deadlock occurs is because two threads depend on accessing the same shared resource, but neither of them can or they're simply waiting for each other indefinitely. To avoid deadlocks, we could try and follow a few simple principles. Each shared resource should have its own lock object. In the example we looked at earlier, this means the lock should only be used when working with a decimal variable total. If we were to introduce more parallel operations to access other shared resources, the best practice would be to introduce a separate lock object for that resource. This also means giving the lock object a more appropriate name other than syncRoot. For instance you could call it totalLock, but don't confuse that with something that totally locks the application. Also, according to the Microsoft guidelines, you should avoid using the following as a lock, a string, a type instance, which you get from a type of call, this, which is a reference to the current instance. This will help you avoid deadlocks when you have more than one shared resource. To give you an example of a deadlock, the most common situation is when you are using nested locks and also requires more than one thread. This is for more advanced scenarios, but it's really not that uncommon. We'll first introduce two locks. The assumption here is that we have two shared resources that we'd like to work with. Now we want two operations to run in parallel. This will both use the two locks, which will eventually end up deadlocking both the operations. In the first operation, we're going to lock the first lock. Inside the lock context, we'll perform some operation. We can use Thread.Sleep with a very short sleep to simulate some work. At a glance, there's a very minimum amount of work happening inside the locked context, meaning we're following the best practices that we've previously discussed. After that is completed, we'll go ahead and lock the second lock to interact with another shared resource. To keep it simple, let's just print something to the console. The first operation is now introduced. And simultaneously, we want to run another operation in parallel. This operation requires to first lock the second lock. Then go ahead and run some operation. And again we'll use Thread.Sleep to simulate this. After this is completed, we'll try and lock the first lock and then print something to the console. Can you already spot why this is going to be a problem? If we just look through the code and what's happening here, we can see that the first operation or thread will lock the first object. The second operation, which will start almost at the same time, will lock the second lock object. When both of the first Thread.Sleeps are completed, they try to run the second piece of the code. But as the locks are already locked, they're going to wait until that's available. But given the fact that both of these threads are waiting for each other, that will never happen. This means both the threads will be locked. And guess what? The locks are never going to be released because these operations can never complete. Hence, these parallel operations deadlocked. Let's just run this to confirm, and we can use Task.WhenAll at the end of the method to just make sure that we await these operations. As you can see, it runs forever and doesn't print anything to the console. This proves our theory that it's deadlocked. We know that the work they're both performing only takes about 2 ms worth of work. So clearly, it should have been much quicker, and thus we have a deadlock. While this code shows how to introduce a deadlock, I'm pretty sure that most of us wouldn't intentionally write code like this. However, consider the case when you're consuming a library or calling a method from a locked context. You don't really know if that method internally uses a shared lock, which could then end up causing a deadlock. If you're using nested locks or even calling a method with in your locked context, you should be very careful. In our code, one way of avoiding the deadlock would be to avoid sharing the same lock. In a real‑world scenario, you'd have to be very cautious with how you use a lock, especially if you're introducing nested locks. Follow the guidelines and the best practices that we've discussed, and you should be able to avoid deadlocking your code. Next, we'll talk about canceling parallel operations.

Cancel Parallel Operations
Allowing for an operation to be canceled is a good idea. When it comes to Parallel.For, Parallel.ForEach, and Parallel.Invoke, they all support cancellation out of the box. When it comes to cancellation, these parallel operations do behave slightly different from how the break and stop functionality works. To explore this, let's introduce the cancellationTokenSource. Once we've got this cancellationTokenSource created, we can tell it to automatically cancel after a certain amount of time. To do that, you can call the method CancelAfter with a given number of milliseconds. If you pass 2000, that means it will signal for cancellation after 2 seconds. There's also a constructor overload that lets you pass a timespan, which determines the time before automatically signaling for cancellation. I prefer to use the method CancelAfter for this example as it's much clearer what's going to happen. The cancellationTokenSource creates a CancellationToken. To pass the CancellationToken, you first need to create the ParallelOptions. These options, as you've seen earlier, exposes ways to configure the parallel execution. Set the CancellationToken to the token created by the cancellationTokenSource. Also, set the MaxDegreeOfParallelism to 1 just to easier understand what's going to happen. With this, you're now ready to use the ParallelOptions with the Parallel.For loop. Pass it as the third parameter, and your Parallel.For loop is now configured to monitor for cancellation, as well as only running one thread at a time. Inside the loop body, you can call the method Compute and add the result to the total variable. You can choose if you'd like to use the Interlocked class or if you want to use a lock object that we've explored earlier. I'm going to use the Interlocked class to keep the code to a minimum and accept that I must cast this to an integer, which means I'm certainly going to lose a little bit of data. But that's fine for now. What do you think is going to happen when we run the application? Will the Parallel.For loop automatically cancel? How much will it have increased the total's variable? Let's run the application to find out. After a little while, an OperationCanceledException was thrown. This may come as a surprise. But as previously discussed, the parallel operations will monitor for exceptions. And in this case, it also monitors for cancellation. When it detects a cancellation, it will throw this exception. We still don't know how many of the iterations it executed. It did seem to throw an exception rather quickly. So I'm pretty sure it didn't run through all the iterations. If you wrap the Parallel.For call in a try and catch block, we can catch the OperationCanceledException and write something to the console when that occurs. Then, at the end of the method, make sure that you also print the result of the total's variable. Run the application again. This time, the result will be a little bit different. And there are two things that I find rather interesting. First, we can see that the application completed executing after approximately 2 seconds, which is, in fact, exactly the amount of time we told the cancellationTokenSource to cancel after. Second, the result of the totals variable is not nearly the same value as we've previously gotten when running the application. Therefore, we can assume that when the cancellation was requested, no further iterations were started. This is different from what you might have expected. As we saw previously when exceptions were thrown inside the iterations, the subsequent iterations weren't automatically canceled. However, in this case they were. This is because it's built on top of the Task Parallel Library, and it makes sense that no new tasks are started because the CancellationToken is set to cancelled. An important thing to remember is that it won't stop the execution of already started operations. If that's important, you will have to monitor the CancellationToken inside the iteration and handle that properly just like with a normal task. This is good for situations where you might have more than one expensive operation, and a cancellation could have been requested between the two operations. You then have the option to check IsCancellationRequested or throw an exception. Personally, I always go for checking if the cancellation is requested. It's important to remember that it won't start any new operations when a cancellation is requested. And depending on who runs the application, 2 seconds might mean that it completes the entire operation or it might mean that it only completed one iteration. With that being said, this is a great addition. And in most cases, when a cancellation is requested, the data that has been processed so far is not of interest. Most of the time, the result of a parallel operation is only interesting if the entire operation ran to completion. This allows you to introduce the CancellationToken and the cancellationTokenSource with the parallel operations in the Task Parallel Library.

ThreadLocal and AsyncLocal Variables
Sometimes, you want to have a static variable that's only static for the current thread. This means if you spawn a new thread, the static variable won't contain any value. Another way of looking at this is to provide a container or storage of data that's only available across the thread. Given that the task is an abstraction on top of threading, a ThreadLocal variable may not work as you'd imagine. The idea of a ThreadLocal variable is that the value is available for the given thread. This makes sense and is, in most cases, exactly what you'd want. However, with the Task Parallel Library, this doesn't ensure that each task or parallel operation is executed on a completely new thread. This means that the thread static values may be available in completely newly spawned asynchronous operations because internally the Task Parallel Library will reuse already existing threads. Let's see how this works in action. To create a ThreadLocal variable, we can use the generic class ThreadLocal<T>. The type parameter defines what type of data that you will be storing for the thread. You can use the Value property to set the current value. This means that this value is now attached to the thread that sets the particular value. The ThreadLocal class will keep track of all the threads that have set values on it. This here will be rather interesting to explore. Let's jump over to Visual Studio and introduce a ThreadLocal variable of a nullable decimal. We can now grab the current value assuming that this runs on a new thread for each parallel iteration. This should always be null. Then, we'll set the value to the result of the Compute invocation. Let's also make sure that the degree of parallelism is set rather low. This will make it easier to follow the execution. If you set a breakpoint in the Parallel.For loop, we can inspect what happens when this code executes. The first time we reach the breakpoint, obviously Value is going to be null. As we run the next iteration, it seems that this value is still null. What do you think happens now that we execute this a third time? The threadLocal storage now has a value. This proves that the task in the Task Parallel Library will reuse a thread that was previously completed. It also means that you can't trust data inside your ThreadLocal variables and assume that you're on a unique thread when using the Task Parallel Library. As an alternative to ThreadLocal, there's another type called AsyncLocal. This provides a container that enables you to store data that's available through an entire asynchronous process. Now this sounds a little bit more like what we'd be after in a situation where we use the Task Parallel Library. If you just change ThreadLocal to AsyncLocal, as well as MaxDegreeOfParallelism to 1 and run the application again, you can see that each iteration will have the previous value from the previous iteration. What do you think's going to happen if we change the anonymous method to an asynchronous anonymous method. Then, each iteration will be its own asynchronous context and therefore have its own representation of that AsyncLocal container. To make it even more interesting, what do you think is going to happen if you set an initial value right before the parallel operation. And let's also read this value after the operation is completed. Let's now see how this behaves. Set a few breakpoints in the application and run this with the debugger. Now, each time you enter the asynchronous anonymous method, the initial value of the AsyncLocal variable will be what you set right before the parallel operation. Each parallel operation is then trying to update the value. As you see, if you step over the call to Compute, the value in the AsyncLocal variable was updated. But if we now release the debugger, you can see that after the parallel operations completed, the AsyncLocal variable is back to its original value, which we set right before the parallel operation started. This is because when you write to the AsyncLocal variable in a method you call which is marked as async, it won't write back to the original AsyncLocal variable. It will, in fact, keep a local version and only read the initial value. I suggest that you experiment with this and try to introduce a chain of asynchronous or parallel operations and see how the AsyncLocal variable behaves. This is extremely powerful and allows you to specify a container of values that's only local to your asynchronous context. It's important to understand the limitations of the ThreadLocal, as well as AsyncLocal variables. If you need to apply these in your applications, now you won't be surprised when you see how they work in the real world.

Summary
You've seen how powerful the Task Parallel Library is, as well as the parallel operations that it provides. With these advanced topics that we've covered throughout this module, you now have a better understanding of how to build some powerful and better‑performing applications. You started off learning about sharing variables between parallel operations. This showed you that it's extremely important to be cautious when introducing a lock and a lock object. Sharing variables is often needed, but without the understanding of the problem a lock can introduce, it may become rather difficult. Now, you know that a lock should only do the very bare minimum and most of the work should be performed outside of the lock statement. This will ensure that your code runs as fast as possible. Running too much code inside a locked context may, in fact, make your application perform as bad as an application without parallel principles. Once you understood how to share variables, we looked at performing atomic operations in a thread‑safe manner, this together with the Interlocked class. Of course, this requires that you work with integers, which, in our case, resulted in a bit of data loss as we had to cast the decimal to an integer. The Interlocked class is not for every use case. However, if you can, I would strongly suggest that you use this instead of a lock object. You also learned about deadlocking, why that occurs in parallel operations, and now you know that it happens when you have a nested lock. It's a bit trickier to get a deadlock in comparison to when using async and await. Knowing why it happens and how to prevent that will allow you to structure your code to avoid this. A good rule of thumb is to avoid sharing lock objects. You then learned how to cancel a parallel operation. If you request a cancellation, no further operations will be started, and an OperationCancelledException will be thrown. This is a little bit different from how stop and break works as those had to be manually handled. With the understanding of the Task Parallel Library, it shouldn't come as a surprise that a task won't start if you pass a CancellationToken marked as cancelled. This is exactly what's happening internally when using Parallel.For, ForEach, and Invoke. Remember though, it won't cease the current executions. It just won't start new ones. If you have to roll back data, you'd obviously have to handle that on your own. Finally, we talked about ThreadLocal and AsyncLocal variables. They're completely different and do behave rather differently from each other. We saw that ThreadLocal variables may flow across different tasks because internally, the task reuses threads. We also saw that AsyncLocal variables creates a copy per asynchronous context, if you write to it that is. You can set the AsyncLocal value, and it flows to subsequent calls. But if those asynchronous methods change that variable, a local copy will be created for that context. Both the ThreadLocal and AsyncLocal types are extremely powerful and useful. It's important to understand how they work and not be surprised by the fact that threads are shared between different tasks. With all of this covered, you're definitely ready to tackle parallel programming and build more powerful and better performing applications. In the next module, we are going to explore the last piece of the Task Parallel Library, which is parallel LINQ. If you already use LINQ in your applications to process collections of data, applying the parallel principles to that may result in a much faster application. So continue to the next and final module to learn about that.

Using Parallel LINQ (PLINQ)
Introducing Parallel LINQ and How to Best Use It
Welcome back to the course Asynchronous Programming in C#. I'm Filip Ekberg, and now we are going to look at Parallel LINQ. This module requires that you understand the Language Integrated Query, also known as LINQ. We're going to explore how to use LINQ to perform parallel processing. You can use LINQ to write operations that filters, converts, sums, or works with collections of data in heaps of different ways. The goal is to apply a parallel layer to vastly improve the performance of that operation. This is where parallel LINQ is introduced. Parallel LINQ will work with both the query, as well as the method syntax. PLINQ, or Parallel LINQ, is a parallel implementation of the Language Integrated Query, meaning that you can take some LINQ code and very easily change that to execute in parallel. Depending on what the query is doing, this may or may not run in parallel. Whether or not it does, depends on an internal analysis of that query. A prerequisite, of course, as we've discussed many times now, is that each operation needs to be completely independent. To start writing parallel queries, you first need a collection of data to work with. Once you've got an IEnumerable<T>, you can simply call AsParallel, and this will create a parallel query. To execute the parallel operation, you could simply call a method like Sum or ToList or use a ForEach loop. In this example, you have a short list of elements. Each element will be passed to an expensive operation for it to be processed. When this is run sequentially, it uses one thread for all the processing. Let's say that it takes a few seconds to complete. If you simply add AsParallel, the query will instead run as efficiently as it possibly can on the current system. The operation will then use all the available resources very much like we've seen previously with the parallel operations. The result will therefore complete much faster. The parallel query produced by the call to AsParallel will expose functionality to configure how the query will execute. For instance, the ParallelQuery class exposes methods for setting a cancellation token and the degree of parallelism, as well as a bunch of other useful configuration capabilities. Each of these different methods return the configured parallel query. This means that you can build really beautiful method chains. When the parallel query is created, you should know that this does add a fair bit of overhead as it needs to be able to analyze the query that you are going to perform. Keep in mind that it needs to handle all the threads or tasks internally and also make sure that it runs as fast as possible. This means that you shouldn't overuse AsParallel and simply think that everything will run faster. It may, in fact, result in more overhead and an overall slower application. If you have a parallel query and you'd instead want that to perform sequentially, meaning you don't want any parallel processing, you can retrieve an IEnumerable by calling the method as sequential, which means that it will then allow you to sequentially process data. This can be combined with AsParallel. This will allow you to perform parts of the query in parallel and other parts of it in sequence. As a word of caution, you won't be able to convert every LINQ operation to execute in parallel. It very much depends on the purpose of that operation. When you execute the query after you've added AsParallel, the query will be analyzed before executed. This allows for two important decisions. If the code is faster to execute without parallelism, it will do so. If it's unsafe to perform the operation in parallel, the query will be executed sequentially like a normal LINQ operation. This behavior can be changed by setting the parallel execution mode. You've got two options here. Either you use the default behavior, which will be best for most cases. Or you can force the query to execute in parallel, even if the analysis determines that there's more overhead doing so. You would, of course, only do this if you are absolutely certain that the operation will be performing better overall and that you don't care about the extra overhead and potential implications. With that being said, I would really recommend that you stick with the default behavior. You shouldn't assume that just because you add AsParallel, your queries will run faster. In fact, you will most definitely only see a performance improvement for larger collections where each item needs to run through some time‑consuming operation. Just as we discussed in the previous module, if your code in the parallel operation needs to lock for a long period of time, that won't necessarily perform better than a normal sequential operation. Instead, you'll end up with even more overhead, both memory‑wise and computation‑wise. Therefore, make sure that you avoid locking and overusing shared resources. This isn't just for parallel LINQ, but it's something that you should keep in mind for any situation where you introduce parallel processing. Follow these guidelines and best practices that we've discussed through this entire course when you apply your Parallel LINQ, and your applications should be much more performant. Next, let's look at how to produce a parallel LINQ.

Creating a Parallel Language Integrated Query
You now understand the purpose of using Parallel LINQ, or PLINQ for short. How about we jump into Visual Studio and look at how we can construct a sequential query that is then improved by introducing Parallel LINQ? We'll use the same base for the console application that we experimented with in the previous module. The code will calculate the amount of time it takes to execute the main method. Using this will give you an understanding of how much better or, in some cases, worse the execution of the code gets. As a side note, there are alternative tools to analyze and profile parallel operations. There's an extension to Visual Studio that you can download called Concurrency Visualizer. I'd really recommend you experiment with this on your own. In our console application, I'd like to do sort of the same thing that we've done earlier. But instead of iterating from 0 to 100 with a for loop, I'm going to first generate a list of numbers. We can do this by using Enumerable.Range. This generates a sequence with numbers ranging from 0 to and including 99. This gives us a bit of data to work with. The goal is to call Compute and pass each item from this sequence to that method. Then we'll get the sum of all those operations. This is effectively the same thing we did earlier with the for loop and the lock object. Constructing this LINQ statement using the method syntax makes it clear what we want to achieve. For each element, we select the result from the invocation to compute. Then each of these results will be summarized. As there's nothing in the code that's currently running in parallel, it makes sense that computing this result of 5000 takes a bit of time. In fact, it does take the same amount of time as we've previously seen in the course. To speed things up, we'd like to run the calls to Compute in parallel. This requires us to construct a parallel query, which you can achieve by adding a method call to AsParallel. And you do this right before the Select method call in your query. This means that the Select will operate on a parallel query, and computing these values should, in theory, be much quicker. We've got a fair few elements to process, there are no side effects of calling Compute, and each invocation to compute is completely independent. With all of that in mind, do you think this will run faster? Will this run the query in parallel? How about we run the application to find out? It definitely ran a lot faster. On my machine, this runs about 20 to 25 times faster than the sequential operation. We even got the same result, which means we indeed made sure that we improved the performance of the application. This also means that this query did, in fact, run in parallel. Rather easily, we changed the sequential query to become a parallel query instead. Now, instead of selecting the sum, I'd like to grab the first 10 elements produced by the query. To do that, we use the method Take with the given number of elements that we'd like to grab. If you iterate over each of these elements and print them to the console, which elements do you expect this to print? As you can see here, these elements are not printed in order, which makes a lot of sense because the parallel query will start the operation in parallel, and you have no control over which elements are processed when. However, there is a way to persist the order of elements, and sometimes the order of the original sequence is important. After the call to AsParallel, we can instruct the parallel query that we like to treat this as an ordered sequence. As a word of caution, this will add a bit of overhead. I'd also like to mention that this does not execute anything sequentially. It only means we'll be persisting the order. Another important point to make here is that when adding Take, it won't limit the parallel operation to only 10 elements. Compute will definitely be called more than 10 times. I would suggest that you set a breakpoint in Compute and experiment with that on your own. With AsOrdered being added, this means that when we take the first elements from the result and when this is printed, we can see that we now get 0.5 to 9.5. We persisted the order of the elements. And you know what's great? We did this without any drastic change to the performance. It still takes about the same time to execute. The query is now running in parallel. Each element is being processed by a time‑consuming operation, but we greatly improved the performance. Even if the order is important when we are grabbing the 10 elements, when we then process the 10 elements, that might not have to be done sequentially. After we've gotten the result, we sequentially iterate the sequence using a normal foreach loop. We could, of course, introduce a parallel foreach if we'd like to process this ordered list of items. However, the parallel query also exposes a method to make this a little bit easier for us. Instead of running the foreach loop, you can call the method ForAll together with either an anonymous method or pointing to a method for it to invoke. This will ensure that the query is executed. And for each element in the resulting query, we run the method passed to ForAll. This means we can have a truly parallel processing of the elements. Nothing in the code is now processing the list or the result sequentially. You can see that we still have all the elements even though they were not printed in order. You persisted the order of the elements in your list, but then you could completely process each of the elements in parallel. With this, we've now introduced a parallel query. You can apply this to your own LINQ statements and experiment with how this impacts the performance of the application. As a suggestion, you can now experiment with adding the method WithCancellation as well as WithDegreeOfParallelism to the application. It will behave just as like we've seen previously, and it's a great addition to Parallel LINQ. Converting LINQ to become parallel might seem as easy as adding AsParallel. However, you shouldn't forget that the parallel query will analyze your query and what is performing, and there's no guarantee that it will run the process in parallel. Therefore, you should try and apply this in your particular situation and see how it impacts performance. But as you've learned throughout this course, there's much more that you need to consider to make sure that your applications behave exactly the way you intended to. As mentioned earlier, depending on what your query is doing, it may force to run sequentially. This is something determined by the parallel query. To help you determine this, here's a list from the Microsoft documentation on what to look for and when the parallel query will choose to run sequentially. If you are finding that your parallel queries are choosing to run sequentially, you may only need to refactor your code slightly to make sure that it runs in parallel. You now have a good understanding of how to build multithreaded applications using the different components of the Task Parallel Library.

Summary and Final Words
You've now seen how to use Parallel LINQ from the Task Parallel Library. With this, you can now convert existing queries to run them in parallel. You started off learning about the benefits of the parallel query and what it provides out of the box. With the knowledge you've gathered throughout this course, you can now safely apply parallel principles in your applications, either by using the parallel operations manually using the tasks or even construct queries that use LINQ that runs in parallel. This will make your applications run smoother and hopefully with less issues as you now know how to handle different scenarios. You then saw how to construct your own parallel query, as well as persisting the order of elements. This is, in some cases, important. And being able to persist the order, but still run the entire query in parallel may greatly improve the total execution time. We've covered so much in this course around how to apply asynchronous programming and build multithreaded applications in C# with the help of the Task Parallel Library. By now, I really hope that you have a good understanding of how to work with async and await, the Task Parallel Library, and, of course, it's extensions for parallel operations. All that we want is to provide as fast code as possible to give our users a better experience when using our applications. With all this new knowledge, you are now ready to tackle different situations. If you've got any questions, I'm here to help. You just completed Asynchronous Programming in C#. My name is Filip Ekberg, and thank you for participating in this course.